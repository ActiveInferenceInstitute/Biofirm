https://docs.google.com/document/d/1kM26R0pYUuj_WT5Z2uuwLssFaTO51hfh_R63d4X2h-0/edit?tab=t.0

Biofirm Development Documentation, 10/30/2024
Andrew Pashea


Overview:
We aim to have a working simulation of a program where Active Inference agents perceive and act to reduce expected free energy for a “biofirm” tasked to maintain a positive state of an environment as evidenced by seeing data collected from the environment in target constraint ranges. For this version of the project, we are using a purely synthetic environment with interdependent equations defining the environmental variables’ update rules, which agents will analyze and leverage similarly to how real analysts would study and learn the relationships in the actual data they collect in order to design and perform interventions for realizing desirable outcomes. Some environmental variables can be directly impacted by the agents, thus the necessities for problem-solving and learning the relationships between variables in the data on the part of the agents. By minimizing expected free energy and attaining desirable outcomes, agents generate valued tokens equivalent to the free energy minimized from timestep to timestep. These tokens are redistributed to agents based upon each agent’s marginal contribution to the global expected free energy minimization. These tokens in aggregate compose a “credit coop token reserve”, in which the tokens are accumulated.

The project as currently conceptualized aims to accomplish the following:
First, a PDF document consisting of a USDA report on characteristics of a farmland / natural preservation, including information for maintaining it, was sent to an LLM via an f-string literal. The overall prompt is written with careful instructions, informing the LLM to interpret from the document 10 interdependent variables – 3 of which are “controllable” while the others are dependent upon the controllable variables and one another, and to write a function `initialize_environment()` to create an artificial environment containing these variables with their own respective update functions. The LLM must then write only Python code which can be executed to create the environment. These 10 variables are related and able to be updated via a set of equations, including a noise term, and wrapped in an ‘Environment’ class. `env = initialize_environment()` is used to instantiate the environment, which can be updated at each timestep of a simulation using `env.step(c1,c2,c3)` where the three arguments are numeric values for the three controllable variables. All variable values are stored as attributes in the environment. A pandas DataFrame of the history of all variable values is also stored as an attribute, `env.data`, with an added ‘timestep’ column denoting when the corresponding row of data was added. This historical data can then be used for analytics purposes. 
In future, this step of the process ideally will be adaptable to any PDF (or synthesized set of PDFs). Though there is commented code to directly execute the LLM’s Python code output, via `exec(environment_code)`, for this version the code has instead been copied and pasted into its own cell so that it can be reproduced and used for the current demonstration. In future, there could instead be a “code template” requesting the LLM to fill in its values to increase accuracy of desired output.
In future, instead of an artificial environment, real collected data fit for the purpose at hand with manually set target constraint ranges could be used instead of an artificial environment. 
The environment also contains upper- and lower-bound target constraints per variable, and at each timestep computes a list of binary indicators denoting if the current values of the variables are within (1) or not within (0) their target constraint bounds. These indicators are contained in a list, stored as an attribute `env.constraint_verification`. For example, [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] would indicate that none of the environment’s variables are currently within their target constraints.
Note that were the environment to be updated via `env.step(0,0,0)`, with the arguments of zero denoting that the controllable factors are all zero, this would denote that no intervention has been made to impact the controllable variables, and therefore the uncontrollable variables will update based upon their respective equations with the controllable variable terms as zero.
Next, a “pro forma business model” should be constructed via a carefully crafted LLM prompt, where the LLM is requested to write a pro forma business model for what will become a small biofirm composed of a handful agents all tasked with keeping their environment’s variables within target constraints. Crucially, the prompt should also contain a description of the variables in the environment in terms of their descriptive characteristics and target constraints as well as their data types and structure within the dataframe. The `env.data` dataframe effectively becomes the biofirm’s database which is updated over time and used for making predictions and devising interventions.
Next, agents should be constructed who receive the `env.constraint_verification` list as an observation, thus each agent has 10 observation modalities each containing two discrete observations aligned with the binary indicators. This means agents observe, in a simplistic way, if the environmental variables are within their respective constraints or not.
The agents then infer a single hidden state factor with two levels representing ‘bad’ and ‘good’ as an overall evaluation of the state of the environment they are meant to take care of. Therefore the agents prefer ‘1’ observations and dysprefer ‘0’ observations for all modalities. Agents should have a small number of policies available related to exploring and testing hypotheses regarding how the variables in their environment relate and how to set targets for the three controllable variables. The policies may relate to using external models, e.g., using a function which takes `env.data` as an argument, builds and trains a Bayesian and/or ML model using the data, and makes a prediction for one or more controllable variables. Agents could be equipped with an additional “NULL” policy meaning they do nothing, in which case they proceed to use the same model or prediction process without making any adjustments. Or, agents might use an LLM to explore a relationship between two or more variables by submitting their pro forma business model, in addition to other previous LLM responses and/or metrics attained from previous prediction models, to an LLM to request it write code 
In future, if this construction is deemed feasible, this will allow for adjusting the target constraint ranges, e.g., by the user, without necessitating a change in the agents’ observation modalities. 
Now the action-perception loop can begin. The agents will receive the observations `env.constraint_verification`, infer the hidden state factor related to the quality of the environment, infer policies, and sample and commit actions which impact the environment with the goal being to modify the biofirm’s controllable variable inputs into the `env.step(c1,c2,c3)` function. The agents are acting to achieve preferred outcomes equal to the ‘1’ binary indicators, which will be achievable by choosing values for c1, c2, and c3 which lead to updates to the environment that meet environmental constraints.   Finally, the environment is updated at the end of each timestep using the agents’ determined c1, c2, and c3. 
The updated `env.constraint_verification` is sent to a higher-order agent representing the biofirm, which itself does not act (it is programmed with a single “do nothing” action) and is instead used to infer the overall quality of the environment just as the aforementioned agents do. The biofirm agent can be reset at each time step and is used as a perceptual model, where its expected free energy term is tracked. 
An approach that can track each lower-order agent’s contribution to minimizing the biofirm agent’s expected free energy should be employed such that Shapley values can be computed to determine each agent’s marginal contribution to the higher-order biofirm agent’s expected free energy minimization. Then, the current expected free energy of the biofirm minus the expected free energy from the previous timestep is subdivided based on each agent’s marginal contribution. There is then a token creation logic: "Tokens are awarded to different agents (two [or more] in this case) and for how much their actions, observations, models reduced FE [i.e. expected free energy] - and then have the Shapley Value allocate the appropriate reward. These tokens would [be] awarded through each successful perception, prediction, update cycle - and then accumulated in the “credit coop token reserve”- essentially in this case just a pooling of the tokens being generated reflecting the learning of the agent."
The simulation repeats over timesteps until some specified timestep T is reached.
Additional logics to incorporate and have active over time include:
Tracking the LLM prompts and responses as strings, e.g., within a dict or other local object.
Updating the biofirm pro forma business model when some detrimental condition(s) is met, for example if X number of binary indicators reach zero simultaneously.
Tracking various components, e.g., environmental data history, constraint verification history, agent actions taken history, significant events during the timesteps, previous and current pro forma business models, etc. These various components can then be compounded/concatenated into a structured string that can be saved to a PDF file at the end of the simulation to provide interpretable information about the simulation and its results over time.
For a demonstrable simulation, target might be 20 to 30 timesteps, perhaps with an external environmental perturbation or two, say, at t=15 or t=20, etc., to see how agents deal with the “shock”.





Progress so far, where numbers align with the steps of the aforementioned process:
Artificial environment (step 1) has been generally accomplished. 
“Previous code” has been written to make attempts at pro forma business models, which is very doable. Interesting ways of having it created such that it actually influences agent policy construction would be a plus, otherwise it can act as a placeholder for a demonstration.
This will be the primary bulk of the remaining work, and more specifically designing and writing out functions and other code which correspond to agent policies. Agents might have strictly separated policies (“make a prediction using a ___ model for controllable factor ____”, “ask for more information”, ….). Maybe there should be 3 agents, each mapped to a particular controllable factor they take care of specifically. Maybe there should be a conversational pool where agents communicate in natural language via LLM, albeit aim for now is to both focus on the primary aspects of the simulation to emphasize in a demonstration, as well as keeping a mind to reducing run-time and cost by avoiding excessive LLM prompting.
This will be the area where the logics involved in agents’ policies and how they impact the environment are defined and carried out.
The concept has been partially achieved, albeit clearly delineating each agent’s respective contribution to EFE minimization requires more nuance. One idea is to have each agent responsible for one controllable variable, and feeding in `env.constraint_verification` as well as variations of it where each agent’s indicator is necessarily zero. This might require significantly more nuance; or, some more sophisticated function for computing free energy (which itself ideally would be expected free energy – a form of free energy impacted by preferences of the biofirm agent), or some entirely separate function written out which can take variable values from the data directly.
See above.
Simply add and test code along the way in the action-perception loop which captures these metrics and stores them in objects in a coherent manner. The token generation dimension should be simple enough if all of the aforementioned components are in place.
