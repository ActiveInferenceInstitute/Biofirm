# -*- coding: utf-8 -*-
"""ActInf BioFirm v_011.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tSkBQSYJU32CrFz22v22ARNDwQMrtQvn

# OpenAI initial setup
- Get account at [openai.com](http://openai.com) (Google sign, manual account creation, etc.)
- https://platform.openai.com/playground?mode=chat → log in → Dashboard (upper-right) →  API keys (left) → Create new secret key → can be used at PATH variable (recommended) or hard-coded into your script (insecure)
  - direct link: https://platform.openai.com/usage
- will want to add funds to account

# Running previous chat with AI for help/code snippets:
- https://www.perplexity.ai/search/how-do-i-get-an-api-key-for-us-PfC26UOWTxi92by2EVuENQ
"""

YOUR_API_KEY = ""     # input your OpenAI key here

!pip install openai
!pip install inferactively-pymdp
!pip install PyPDF2
!pip install pdfplumber
!pip install tiktoken

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pymdp
from pymdp import utils
from pymdp.maths import softmax
from pymdp.agent import Agent
import copy
import math
import random

np.random.seed(0)
random.seed(0)

import openai

import pdfplumber

def extract_pdf_text_plumber(pdf_path):
    pdf_text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            pdf_text += page.extract_text()
    return pdf_text

# Example usage
pdf_path = "/content/drive/MyDrive/Colab Notebooks/pymdp/biofirm_data/A. CPA - DIA_Clippinger 2024.pdf"
try:
    extracted_text = extract_pdf_text_plumber(pdf_path)
    print("PDF content extracted successfully!")
except Exception as e:
    print(f"An error occurred: {str(e)}")

import tiktoken

def count_tokens(text):
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    return len(encoding.encode(text))

count_tokens(extracted_text)


#print(extracted_text)

variables = 10
controllable_variables = 3

# #DO NOT EDIT: working
# YOUR_API_KEY = YOUR_API_KEY
# client = openai.OpenAI(api_key=YOUR_API_KEY)


# variables = 10
# controllable_variables = 3

# def interpret_environment(text, variables, controllable_variables):
# 	prompt = f"""We are designing an artifical environment in Python in which reinforcement learning agents interact. The reinforcement learning agents will have goals to maintain
#   and care for this environment in a sustainable way by acting to ensure key environmental variables stay within defined constraints. We have a document which inspires this environment as follows:
#   `{text}`. Using this text, do the following and ensure that your response only contains code and comments which can be executed in Python:
#   1. Decide on {variables} variables which represent our environment, as relate to the topics and concerns in the document. {controllable_variables} of the variables must be controllable by the agents, relate to realistic scarce resource management, and impact other uncontrollable variables in a realistic way. Each of the uncontrollable variables should have a relatively small, realistic target range as a constraint in which we prefer the variable's value to be.
#   2. Write a Python function `initialize_environment` to initialize a class `env` containing an artificial environment where these 10 variables are updated at each timestep. The update rules for the variables must have logical interdependencies between them such as positive and negative correlations with a small amount of noise included. The function should include a `step` method which takes {controllable_variables} arguments representing the controllable variables so that, at each timestep, the environment can be updated based on the controllable variables inputs and their impacts on the uncontrollable variables. The function should return `env` containing all of the variables and their values. The function should also include a `print_env` method which prints the current values of the environmental variables. The function should also store all of the data for each time step in a pandas dataframe stored as an attribute `data` where the first column presents the timestep with column name 'timestep' and the remaining columns represent the values of the environmental variables, and you must use `self.data = pd.concat([self.data, new_data], ignore_index=True)` to append each new timestep of data to the dataframe because `DataFrame` objects have no attribute `append` therefore you cannot use `self.data.append`. `initialize_environment` should already produce one timestep of data, where the variable values are generally poor and not meeting their constraints, and these values should be what initializes the dataframe. This way, our reinforcement learning agents can try to learn which values of the controllable variables will satisfy the environmental constraints through trial and error. Also include an attribute in the environment `constraint_verification` stores and updates a list containing binary values indicating whether the current variables are within their respective constraints, where 0 indicates the variable is not within its constraint and 1 indicates the variable is within its constraint, and where the order of the elements in the `constraint_verification` list are in the same order as the columns in the dataframe (excluding the timestep column). The environment should have an attribute `constraints` containing a single fixed dataframe  with three columns, where the first column 'variable' contains the string name of a variable, the second column 'lower_constraint' contains the lower bound of the variable's contraint, and 'upper_constraint' contains the upper bound of the variable's constraint. The environment should also have an attribute `controllable_variables_list` which contains a list of strings which are the names of the controllable variables.
#   3. Add comments to each part of the code to ensure that we can understand your code, including descriptions of each environmental variable. The controllable variables should include the string 'Controllable - ' at the start of their respective comment.'
#   4. Do a final a review of the code, cleaning it so that your response only has executable Python code. Do not wrap the code in markdown in tick marks. Ensure there are {variables} variables total where {controllable_variables} variables are controllable. Ensure your code only uses pd.concat() to append new dataframe rows and does not try to use `data.append` for the dataframe because there is no .append method for pandas dataframes and trying to include this will produce an error. Ensure the dataframe `data` is initialized to include the initial values of the variables, as it will throw a warning if you try to initialize an empty dataframe.
#   5. Finally, define a local dict object called `environment_description` which describes each of the variables and how they relate to the document provided. Each key should be the name of a variable and each value should be two sentences, where the first sentence describes the variable, its data type, and how it is measured while the second sentence concisely states the target range constraint for that variable. If the variable is controllable, add a third sentence stating the variable is controllable and how an agent could control this variable."""
# 	response = client.chat.completions.create(
#       		model="gpt-4o-mini",
#       		messages=[
#         		{"role": "system", "content": f"You are a skilled Python programmer who specializes in leveraging reinforcement learning and LLMs for designing simulations to gain insight into business and management operations."},
#         		{"role": "user", "content": f"""{prompt}"""}
#       		],
#       		temperature=0.9,  # controls randomness vs. coherence
#       		seed = 1
#   	)
# 	response = response.choices[0].message.content
# 	return response

# environment_code = interpret_environment(text=extracted_text, variables=variables, controllable_variables=controllable_variables)
# print(environment_code)

# Query run on 10/28/2024

import pandas as pd
import numpy as np

def initialize_environment():
    class Environment:
        def __init__(self):
            # Initialize environmental variables with poor initial values
            self.forest_health = 40  # Forest health index (0-100)
            self.carbon_sequestration = 10  # Carbon storage (tons per acre)
            self.wildlife_habitat_quality = 30  # Scale from 0-100
            self.water_quality = 60  # Water quality index (0-100)
            self.soil_health = 40  # Soil health index (0-100)
            self.invasive_species_count = 100  # Count of invasive species (number)
            self.riparian_buffer_width = 10  # Width of riparian buffer (feet)
            self.biodiversity_index = 50  # Biodiversity score (0-100)
            self.hazard_trees_count = 20  # Count of hazard trees (number)
            self.recreational_access_score = 20  # Score from 0-100 (access quality)

            # Store variable names in a list for manageable access
            self.variable_names = [
                'forest_health',
                'carbon_sequestration',
                'wildlife_habitat_quality',
                'water_quality',
                'soil_health',
                'invasive_species_count',
                'riparian_buffer_width',
                'biodiversity_index',
                'hazard_trees_count',
                'recreational_access_score'
            ]

            # Create an initial data frame to store environmental data over timesteps
            self.data = pd.DataFrame({
                'timestep': [0],
                'forest_health': [self.forest_health],
                'carbon_sequestration': [self.carbon_sequestration],
                'wildlife_habitat_quality': [self.wildlife_habitat_quality],
                'water_quality': [self.water_quality],
                'soil_health': [self.soil_health],
                'invasive_species_count': [self.invasive_species_count],
                'riparian_buffer_width': [self.riparian_buffer_width],
                'biodiversity_index': [self.biodiversity_index],
                'hazard_trees_count': [self.hazard_trees_count],
                'recreational_access_score': [self.recreational_access_score]
            })

            # Initialize constraints for each variable
            self.constraints = pd.DataFrame({
                'variable': [
                    'forest_health',
                    'carbon_sequestration',
                    'wildlife_habitat_quality',
                    'water_quality',
                    'soil_health',
                    'invasive_species_count',
                    'riparian_buffer_width',
                    'biodiversity_index',
                    'hazard_trees_count',
                    'recreational_access_score'
                ],
                'lower_constraint': [50, 30, 50, 70, 60, 0, 25, 60, 0, 30],
                'upper_constraint': [100, 100, 100, 100, 100, 50, 100, 100, 15, 100]
            })

            # List to keep track of whether each variable is within constraints
            self.constraint_verification = [0] * len(self.variable_names)

            # List of controllable variables
            self.controllable_variables_list = ['forest_health', 'carbon_sequestration', 'riparian_buffer_width']

        def step(self, controllable_health, controllable_carbon, controllable_buffer):
            # Update controllable variables based on agent inputs
            self.forest_health += controllable_health
            self.carbon_sequestration += controllable_carbon
            self.riparian_buffer_width += controllable_buffer

            # Update uncontrollable variables based on dependencies
            self.wildlife_habitat_quality = np.clip(self.forest_health * 0.5 + np.random.normal(0, 5), 0, 100)
            self.water_quality = np.clip(self.riparian_buffer_width * 1.5 + np.random.normal(0, 5), 0, 100)
            self.soil_health = np.clip(self.forest_health * 0.6 + np.random.normal(0, 5), 0, 100)
            self.invasive_species_count = np.clip(self.invasive_species_count - controllable_buffer + np.random.randint(-5, 5), 0, None)
            self.biodiversity_index = np.clip(self.wildlife_habitat_quality * 0.4 + np.random.normal(0, 5), 0, 100)
            self.hazard_trees_count = np.clip(self.hazard_trees_count + np.random.randint(-2, 3), 0, None)
            self.recreational_access_score = np.clip(self.recreational_access_score + np.random.randint(-3, 3), 0, 100)

            # For the time step, append data to the dataframe
            new_data = pd.DataFrame({
                'timestep': [len(self.data)],
                'forest_health': [self.forest_health],
                'carbon_sequestration': [self.carbon_sequestration],
                'wildlife_habitat_quality': [self.wildlife_habitat_quality],
                'water_quality': [self.water_quality],
                'soil_health': [self.soil_health],
                'invasive_species_count': [self.invasive_species_count],
                'riparian_buffer_width': [self.riparian_buffer_width],
                'biodiversity_index': [self.biodiversity_index],
                'hazard_trees_count': [self.hazard_trees_count],
                'recreational_access_score': [self.recreational_access_score]
            })
            # Update the data attribute with the new data
            self.data = pd.concat([self.data, new_data], ignore_index=True)

            # Verify constraints for each variable
            self.constraint_verification = [
                1 if self.constraints.loc[self.constraints['variable'] == name, 'lower_constraint'].values[0] <= getattr(self, name) <= self.constraints.loc[self.constraints['variable'] == name, 'upper_constraint'].values[0] else 0
                for name in self.variable_names
            ]

        def print_env(self):
            # Print current environmental variable values
            print(self.data.iloc[-1])

    # Initialize and return the environment instance
    env = Environment()
    return env

# Description of each environmental variable
environment_description = {
    'forest_health': "Forest health index (int) measured on a scale from 0 to 100. The target range is between 50 and 100 inclusive.",
    'carbon_sequestration': "Carbon storage (int) in tons per acre, which signifies the amount of carbon captured by the forest. The target range is between 30 and 100 tons per acre inclusive.",
    'wildlife_habitat_quality': "Wildlife habitat quality score (int) measured on a scale from 0 to 100, indicating the suitability of the environment for wildlife. The target range is between 50 and 100 inclusive.",
    'water_quality': "Water quality index (int) from 0 to 100, assessing the cleanliness and health of the water in the environment. The target range is between 70 and 100 inclusive.",
    'soil_health': "Soil health index (int) from 0 to 100, reflecting the quality and sustainability of the soil. The target range is between 60 and 100 inclusive.",
    'invasive_species_count': "Count of invasive species (int), indicating the number of unwanted species present in the environment. The target range is from 0 to 50.",
    'riparian_buffer_width': "Width of the riparian buffer (int) in feet, important for maintaining ecosystem health along waterways. The target range is between 25 and 100 feet inclusive.",
    'biodiversity_index': "Biodiversity score (int) from 0 to 100, which assesses the variety of species in the ecosystem. The target range is between 60 and 100 inclusive.",
    'hazard_trees_count': "Count of hazard trees (int), which are trees that pose a risk to the ecosystem or nearby structures. The target range is from 0 to 15.",
    'recreational_access_score': "Recreational access score (int) on a scale from 0 to 100, indicating how accessible the land is for recreational activities. The target range is between 30 and 100 inclusive."
}

#exec(environment_code)    # use when running querying to generate `initialize_environment()`

env = initialize_environment()
controllable_initial_values = []
# for controllable_variable in env.controllable_variables_list:
#   controllable_initial_values
for i in range(10):
  print(env.constraint_verification)
  env.step(0,0,0)

display(env.data)
print(env.constraints)
print(env.constraints.to_dict())
print("")
print(env.data.to_dict())
print("")
print(env.controllable_variables_list)

print(env.data.columns)

def plot_data(data, constraints, controllable_variables_list):
    # Calculate number of rows and columns for subplots
    n_vars = len(data.columns) - 1  # Exclude timestep column
    n_cols = 5  # Set number of columns as 5
    n_rows = int(np.ceil(n_vars / n_cols))  # Calculate number of rows needed

    # Create figure and subplots
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3*n_rows))
    axes = axes.flatten()

    # Plot each variable
    var_idx = 0
    for col in data.columns:
        if col != 'timestep':
            ax = axes[var_idx]

            # Plot the variable
            ax.plot(data['timestep'], data[col], '-', linewidth=2)

            # Add constraints if they exist for this variable
            if col in constraints['variable'].values:
                constraint = constraints[constraints['variable'] == col].iloc[0]
                ax.axhline(y=constraint['lower_constraint'], color='r', linestyle='--', alpha=0.8)
                ax.axhline(y=constraint['upper_constraint'], color='r', linestyle='--', alpha=0.8)
            if col in controllable_variables_list:
              controllable = " (Controllable)"
            else:
              controllable = ""

            # Customize subplot
            #ax.set_title(col.replace('_', ' ').title())
            ax.set_title(f"{col.replace('_',' ').title()}{controllable}")
            ax.set_xlabel('Timestep')
            ax.grid(True, alpha=0.3)

            var_idx += 1

    # Remove any empty subplots
    for idx in range(var_idx, len(axes)):
        fig.delaxes(axes[idx])

    # Adjust layout
    plt.tight_layout()
    plt.show()

plot_data(env.data, env.constraints, env.controllable_variables_list)



"""### BioFirm 'agent'"""

A = utils.obj_array(variables)
for i in range(len(A)):
  A[i] = np.zeros( ( 2,2 ))    # initialize likelihood between 2 possible observations, 2 hidden state levels ('bad','good')
  A[i][0,0] = 1.0              # P(o='not in constraint'|s='bad') = 1.0
  A[i][1,1] = 1.0              # P(o='within constraint'|s='good') = 1.0
  #print(f"A[{i}] (normalized = {utils.is_normalized(A[i])}) = {A[i]}")
print(f"A ({utils.is_normalized(A)})")
B = utils.obj_array(1)
B[0] = np.zeros( (2, 2, 1 ))
for i in range(len(B[0])):
  for j in range(len(B[0])):
    B[0][i,j] = 0.5           # uncertainty over impact of 'do nothing' to impact hidden state (i.e. irrelevant)
print(f"B[0] ({utils.is_normalized(B[0])} = {B[0]}")
C = utils.obj_array(variables)
for i in range(len(C)):
  C[i] = np.array([0, 1])    # always prefer 1
print(f"C ({utils.is_normalized(C)}) ")
D = utils.obj_array(1)
D[0] = np.array([0.5, 0.5])
print(f"D ({utils.is_normalized(D)}) ")

biofirm = Agent(A=A, B=B, C=C, D=D, inference_algo='MMP',policy_len=1,inference_horizon=2,sampling_mode='full',action_selection='stochastic')

# obs = env.constraint_verification
# qs = biofirm.infer_states(obs)
# print(biofirm.F)

#for i in range(10):
  #print(env.constraint_verification)
  #env.step(random.randint(1, 5),random.randint(1, 5),random.randint(1, 5))
  #obs = env.constraint_verification

import copy
obs = [0,0,0,0,0,0,0,0,0,0]

g_pos_list = []
obs_list = []

for i in range(5):
  for j in range(2):
      obs = copy.deepcopy(obs)
      obs[i] = 0
      qs = biofirm.infer_states(obs)
      q_pi, neg_efe = biofirm.infer_policies()
      action_sampled_id = biofirm.sample_action()
      print(f"env.constraint_verification = {obs} -> F = {biofirm.F}, G = {neg_efe * -1}")
      g_pos_list.append(biofirm.G[0]*-1)
      obs_list.append(obs)
for i in range(3):
  obs = copy.deepcopy(obs)
  #obs[i] = 0
  qs = biofirm.infer_states(obs)
  q_pi, neg_efe = biofirm.infer_policies()
  action_sampled_id = biofirm.sample_action()
  print(f"env.constraint_verification = {obs} -> F = {biofirm.F}, G = {neg_efe * -1}")
  g_pos_list.append(biofirm.G[0]*-1)
  obs_list.append(obs)
for i in range(1):
  for j in range(10):
    obs = copy.deepcopy(obs)
    obs[j] = 1
    qs = biofirm.infer_states(obs)
    q_pi, neg_efe = biofirm.infer_policies()
    action_sampled_id = biofirm.sample_action()
    print(f"env.constraint_verification = {obs} -> F = {biofirm.F}, G = {neg_efe * -1}")
    g_pos_list.append(biofirm.G[0]*-1)
    obs_list.append(obs)
for i in range(3):
  obs = copy.deepcopy(obs)
  obs[i] = 1
  qs = biofirm.infer_states(obs)
  q_pi, neg_efe = biofirm.infer_policies()
  action_sampled_id = biofirm.sample_action()
  print(f"env.constraint_verification = {obs} -> F = {biofirm.F}, G = {neg_efe * -1}")
  g_pos_list.append(biofirm.G[0]*-1)
  obs_list.append(obs)
print(qs)


print(g_pos_list)
print(len(obs_list))
print(len(g_pos_list))
print(obs_list)

import matplotlib.pyplot as plt
import numpy as np

variable_names = env.data.copy().drop(columns=['timestep']).columns

# Create figure and subplots with shared x-axis
fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 4))

# Top subplot: g_pos_list line plot
timesteps = list(range(len(g_pos_list)))
ax1.plot(timesteps, g_pos_list, 'o-', color='blue')
ax1.set_ylabel('Expected Free Energy')
ax1.set_title('Expected Free Energy based on Environmental Constraint Indicators')
ax1.grid(True)

# Bottom subplot: Binary indicators scatter plot
for t in range(len(obs_list)):  # Outer loop over timesteps
    for i in range(len(variable_names)):  # Inner loop over variables
        color = 'blue' if obs_list[t][i] == 1 else 'red'
        ax2.scatter(t, variable_names[i], color=color)

ax2.set_xlabel('Timestep')
ax2.set_ylabel('Indicator Variables')

# Adjust layout to prevent overlap
plt.tight_layout()
plt.show()

"""
__________________________________________________________________
# PREVIOUS CODE"""

# # Extract text from pdf
# import PyPDF2

# def extract_pdf_text(pdf_path):
#     # Create a text string to store the content
#     pdf_text = ""

#     # Open the PDF file in binary read mode
#     with open(pdf_path, 'rb') as file:
#         # Create a PDF reader object
#         pdf_reader = PyPDF2.PdfReader(file)

#         # Get the number of pages in the PDF
#         num_pages = len(pdf_reader.pages)

#         # Iterate through all pages and extract text
#         for page_num in range(num_pages):
#             # Get the page object
#             page = pdf_reader.pages[page_num]
#             # Extract text from page and add it to our string
#             pdf_text += page.extract_text()

#     return pdf_text

# # Example usage
# pdf_path = "/content/drive/MyDrive/Colab Notebooks/pymdp/biofirm_data/A. CPA - DIA_Clippinger 2024.pdf"
# try:
#     extracted_text = extract_pdf_text(pdf_path)
#     print("PDF content extracted successfully!")
#     # Now 'extracted_text' contains all the text from your PDF
# except Exception as e:
#     print(f"An error occurred: {str(e)}")

import openai

YOUR_API_KEY = YOUR_API_KEY
client = openai.OpenAI(api_key=YOUR_API_KEY)

industry = "agriculture"
variable_descriptions = "the weekly average market price of substitute products, the demand for substitute products, and the temperature, which are all continuous values"
environmental_factor = ""
num_agents = 2

def generate_pro_forma(industry,variable_descriptions,num_agents):
	pro_forma_prompt = f"""I am creating an agent-based model of a firm in 	the {industry} industry with {num_agents} departments who all have distinct roles.
	The firm develops one product which is refines based upon market 	conditions.
	The data it has available for predicting sales and revenue are 	{variable_descriptions}.
 	Generate a pro forma business model for this firm, being clear on ethical and safety standards involved in production and distribution as well as a statement which clearly data are available to the agents in terms of Python data types."""
	response = client.chat.completions.create(
      		model="gpt-4o-mini",
      		messages=[
        		{"role": "system", "content": f"You are a skilled business professional in the industry of {industry}"},
        		{"role": "user", "content": f"""{pro_forma_prompt}"""}
      		],
      		temperature=0.7,  # controls randomness vs. coherence
      		seed = 1
  	)
	pro_forma_text = str(response.choices[0].message.content)
	return pro_forma_text

pro_forma_text = generate_pro_forma(industry,variable_descriptions,num_agents)

def generate_agent(pro_forma_text,num_agents,project_agents):
	agent_creation_prompt = f"""

	We are designing agents who represent the {num_agents} committees in the following pro forma business model:
	{pro_forma_text}

	The roles we have already filled so far are: {project_agents}

	Use the following form to create an agent who still needs to be	added to the firm based on descriptions of the agents in the pro forma business model. The form begins and ends with a tick mark. The form is written like a Python script, where the comments describe each form section to be filled in and the exclamation marks are place holders for the content to be filled in.
	Return only the filled in form as your response without additional commentary.

	# descriptive name of agent using underscores instead of spaces to separate words with no more than 3 words used
	agent= "!"

	# the name of the agent filled in the last section followed by a colon and then a space and then a three sentence description of the purpose of the agent
	description= "!"

	# fill in descriptions of the 5 actions available to agent to accomplish their tasks, one sentence per action, each of which can contribute
	action1= "!"
	action2= "!"
	action3= "!"
	action4= "!"
	action5= "!"
	# fill in meaningful python code functions which can be used by the agents to accomplish each corresponding task
	action1code= "!"
	action2code= "!"
	action3code= "!"
	action4code= "!"
	action5code= "!"
	# three sentence description of how the agent will use the available data as stated in their business model to accomplish their task
	observations= "!"
"""
# !!!! NOTE: just added the prompt section regarding action1code, etc.

	response = client.chat.completions.create(
      		model="gpt-4o-mini",
      		messages=[
        		{"role": "system", "content": f"You are a skilled business professional in the industry of {industry}"},
        		{"role": "user", "content": f"""{agent_creation_prompt}"""}
      		],
      		temperature=0.7,  # controls randomness vs. coherence
      		seed = 1
  	)
	response = str(response.choices[0].message.content)

	import re
	extracted = re.findall(r'"(.*?)"', response)
	#agent_list.append(extracted[1])

	return response, extracted



project = {}
project['agents'] = {}
project_agents = []

for agent in range(num_agents):
  response, extracted = generate_agent(pro_forma_text,num_agents,project_agents=project_agents)
  project['agents'][f'n'+str(agent)] = {extracted[0]: response}
  project_agents.append(extracted[1])

for key, value in project.items():
    print(f"{key}: {value}")

# Environment -- https://www.perplexity.ai/search/i-am-writing-python-code-for-a-7ExeYx2OQXq7TQOeIoYfJg

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

class AgriEnvironment:
    def __init__(self, initial_price=70, initial_weather=70, seed=None):
        if seed is not None:
            np.random.seed(seed)

        self.initial_price = initial_price
        self.price = initial_price
        self.weather = initial_weather
        self.state_data = []  # List to collect state information

        # Parameters for environment variable changes
        self.weather_persistence = 0.7
        self.weather_volatility = 2
        self.weather_mean = 67.5  # Midpoint of 50 and 85
        self.price_weather_sensitivity = 0.2  # Price sensitivity to weather changes

    def update(self):
        # Update weather (mean-reverting Ornstein-Uhlenbeck process)
        weather_shock = np.random.normal(0, 1)
        new_weather = (self.weather_persistence * (self.weather - self.weather_mean) +
                       self.weather_mean + self.weather_volatility * weather_shock)
        new_weather = np.clip(new_weather, 50, 85)  # Ensure weather is between 50 and 85

        # Update market price based on weather changes
        weather_change = new_weather - self.weather
        self.price += weather_change * self.price_weather_sensitivity
        self.price = max(1, self.price)  # Ensure price stays positive

        # Update weather
        self.weather = new_weather

        # Calculate demand based on weather and price
        base_demand = 1000
        weather_effect = (self.weather - self.weather_mean) * 15  # Positive effect for warmer weather
        price_effect = (self.initial_price - self.price) * 50  # Negative effect for higher price

        demand_stochasticity = np.random.normal(0, 100)  # Add some randomness to demand

        self.demand = base_demand + weather_effect + price_effect + demand_stochasticity
        self.demand = max(0, self.demand)  # Ensure non-negative demand

        # Collect state information
        self.state_data.append({'price': self.price, 'weather': self.weather, 'demand': self.demand})

    def get_state(self):
        return {
            'price': round(self.price, 2),
            'weather': round(self.weather, 1),
            'demand': round(self.demand, 0)
        }

# Example usage
seed = 42  # Set a random seed for reproducibility
env = AgriEnvironment(seed=seed)
for _ in range(100):  # Simulate 100 time steps for a more detailed view
    env.update()
    print(env.get_state())

# Create a DataFrame from state data
state_df = pd.DataFrame(env.state_data)

# Plotting with shared y-axis for price and weather
fig, ax1 = plt.subplots(figsize=(12, 6))

# Price and Weather plot on the left axis
ax1.set_xlabel('Timestep (Day)')
ax1.set_ylabel('Price / Weather')
ax1.plot(state_df.index, state_df['price'], label='Price', color='blue', linewidth=2, alpha=0.8)
ax1.plot(state_df.index, state_df['weather'], label='Weather', color='orange', linewidth=2, alpha=0.8)
ax1.tick_params(axis='y')

# Demand plot on the right axis
ax2 = ax1.twinx()
ax2.set_ylabel('Demand')
ax2.plot(state_df.index, state_df['demand'], label='Demand', color='green', linewidth=2, alpha=0.8)
ax2.tick_params(axis='y')

# Title and legend
plt.title('AgriEnvironment State Over Time')
lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')

plt.grid(True)
plt.tight_layout()
plt.show()

# environmental health: slight stochasticy but takes production into account +

# Make a "production function" which is determined by ... tech?
# Make a "sell" function determined by ... demand + firm's own prices?

import openai

YOUR_API_KEY = "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"
client = openai.OpenAI(api_key=YOUR_API_KEY)

industry = "agriculture"
variable_descriptions = "the weekly average market price of substitute products, the demand for substitute products, and the temperature, which are all continuous values"
environmental_factor = ""
num_agents = 2

def generate_pro_forma(industry,variable_descriptions,num_agents):
	pro_forma_prompt = f"""I am creating an agent-based model of a firm in 	the {industry} industry with {num_agents} departments who all have distinct roles.
	The firm develops one product which is refines based upon market 	conditions.
	The data it has available for predicting sales and revenue are 	{variable_descriptions}.
 	Generate a pro forma business model for this firm, being clear on ethical and safety standards involved in production and distribution as well as a statement which clearly data are available to the agents in terms of Python data types."""
	response = client.chat.completions.create(
      		model="gpt-4o-mini",
      		messages=[
        		{"role": "system", "content": f"You are a skilled business professional in the industry of {industry}"},
        		{"role": "user", "content": f"""{pro_forma_prompt}"""}
      		],
      		temperature=0.7,  # controls randomness vs. coherence
      		seed = 1
  	)
	pro_forma_text = str(response.choices[0].message.content)
	return pro_forma_text

pro_forma_text = generate_pro_forma(industry,variable_descriptions,num_agents)

def generate_agent(pro_forma_text,num_agents,project_agents):
	agent_creation_prompt = f"""

	We are designing agents who represent the {num_agents} committees in the following pro forma business model:
	{pro_forma_text}

	The roles we have already filled so far are: {project_agents}

	Use the following form to create an agent who still needs to be	added to the firm based on descriptions of the agents in the pro forma business model. The form begins and ends with a tick mark. The form is written like a Python script, where the comments describe each form section to be filled in and the exclamation marks are place holders for the content to be filled in.
	Return only the filled in form as your response without additional commentary.

	# descriptive name of agent using underscores instead of spaces to separate words with no more than 3 words used
	agent= "!"

	# the name of the agent filled in the last section followed by a colon and then a space and then a three sentence description of the purpose of the agent
	description= "!"

	# fill in descriptions of the 5 actions available to agent to accomplish their tasks, one sentence per action, each of which can contribute
	action1= "!"
	action2= "!"
	action3= "!"
	action4= "!"
	action5= "!"
	# fill in meaningful python code functions which can be used by the agents to accomplish each corresponding task
	action1code= "!"
	action2code= "!"
	action3code= "!"
	action4code= "!"
	action5code= "!"
	# three sentence description of how the agent will use the available data as stated in their business model to accomplish their task
	observations= "!"
"""
# !!!! NOTE: just added the prompt section regarding action1code, etc.

	response = client.chat.completions.create(
      		model="gpt-4o-mini",
      		messages=[
        		{"role": "system", "content": f"You are a skilled business professional in the industry of {industry}"},
        		{"role": "user", "content": f"""{agent_creation_prompt}"""}
      		],
      		temperature=0.7,  # controls randomness vs. coherence
      		seed = 1
  	)
	response = str(response.choices[0].message.content)

	import re
	extracted = re.findall(r'"(.*?)"', response)
	#agent_list.append(extracted[1])

	return response, extracted



project = {}
project['agents'] = {}
project_agents = []

for agent in range(num_agents):
  response, extracted = generate_agent(pro_forma_text,num_agents,project_agents=project_agents)
  project['agents'][f'n'+str(agent)] = {extracted[0]: response}
  project_agents.append(extracted[1])

print(pro_forma_text)

for i in range(len(project['agents'])):
  print(project['agents'][f'n'+str(i)])

for agent in range(len(project['agents'])):
  agent_name = list(project['agents']['n'+str(agent)])[0]
  print(agent_name)
  print(project['agents']['n'+str(agent)][agent_name])

# next incorporate the fact that variables 'weather::float64, price::float64, demand::float64' are available as data.
def generate_environment(pro_forma_text,num_agents,project,seed):
  prompt = f"""I am constructing an agent-based model in Python where there are {num_agents} working together in a firm. The firm's pro forma business model is {pro_forma_text} and
  the agents are generally defined as {project['agents']}. Write Python code to create a holistic environment in which these agents work and operate based on the data available to them as described in the business model.
  This code is only for defining the environment, in a way that the environment variables can be updated per time step. If using a random number generator, set the seed to {seed}. Do not define anything additional for the agents themselves nor any type of agent class.
  Write the code in such a way that it can be executed with error and without any additional commentary. Add a comment next to each line of code describing what it do.
  """
  response = client.chat.completions.create(
      		model="gpt-4o-mini",
      		messages=[
        		{"role": "system", "content": f"You are a skilled business professional in the industry of {industry}"},
        		{"role": "user", "content": f"""{prompt}"""}
      		],
      		temperature=0.7,  # controls randomness vs. coherence
      		seed = 1
  )
  return str(response.choices[0].message.content)

env_code = generate_environment(pro_forma_text,num_agents,project,seed)
project['env'] = env_code
print(project['env'])

import numpy as np
import matplotlib.pyplot as plt
from pymdp import utils

# Define the model parameters
num_obs_modalities = 5
num_obs_per_modality = 2
num_states = 2
num_timesteps = 10

# Initialize the likelihood model
likelihood_dist = np.zeros((num_obs_modalities, num_obs_per_modality, num_states))
for i in range(num_obs_modalities):
    likelihood_dist[i] = np.array([[1.0, 0.0],
                                   [0.0, 1.0]])

# Initialize the prior
prior = np.array([0.5, 0.5])

# Initialize the variational free energy function
def vfe(qs, obs, likelihood, prior):
    joint = np.prod([likelihood[m, o, :] for m, o in enumerate(obs)], axis=0) * prior
    # Add small epsilon to avoid log(0)
    epsilon = 1e-10
    return qs.dot(np.log(qs + epsilon) - np.log(joint + epsilon))

# Initialize the posterior
qs = np.array([0.5, 0.5])

# Initialize arrays to store results
vfe_history = np.zeros(num_timesteps)
qs_history = np.zeros((num_timesteps, num_states))

# Run the simulation
for t in range(num_timesteps):
    # Generate "high" observations for all modalities
    obs = np.ones(num_obs_modalities, dtype=int)

    # Compute the variational free energy
    F = vfe(qs, obs, likelihood_dist, prior)
    vfe_history[t] = F
    qs_history[t] = qs

    # Update the posterior (simplified update rule)
    likelihood = np.prod([likelihood_dist[m, o, :] for m, o in enumerate(obs)], axis=0)
    qs = utils.norm_dist(likelihood * prior)

# Plot the results
plt.figure(figsize=(12, 3))

plt.subplot(1, 2, 1)
plt.plot(vfe_history)
plt.title('Variational Free Energy')
plt.xlabel('Timestep')
plt.ylabel('VFE')

plt.subplot(1, 2, 2)
plt.plot(qs_history)
plt.title('Posterior Probabilities')
plt.xlabel('Timestep')
plt.ylabel('Probability')
plt.legend(['Low', 'High'])

plt.tight_layout()
plt.show()

print(f"Final posterior: {qs}")
print(f"Final VFE: {vfe_history[-1]}")

"""# Free energy minimization
- https://www.perplexity.ai/search/summarize-the-code-for-how-fre-oPr.QmvISk6wrdmDcgiS8g

## Free Energy Minimization in Active Inference for Continuous State Spaces

In active inference for continuous state spaces, the free energy is minimized to find the optimal posterior distribution. The key equations are:

1. **Free Energy (F)**:
   The free energy is defined as the difference between complexity and accuracy:

   $$F = \text{Complexity} - \text{Accuracy}$$

2. **Complexity**:
   This term represents the KL divergence between the approximate posterior and the prior:

   $$\text{Complexity} = \frac{1}{2} \left( \pi_p (\mu - \mu_p)^2 + \ln(\pi_p) - \ln(\sigma) + \frac{\sigma}{\pi_p} - 1 \right)$$

   Where:
   - $\mu$ is the posterior mean
   - $\sigma$ is the posterior variance
   - $\mu_p$ is the prior mean
   - $\pi_p$ is the prior precision (inverse variance)

3. **Accuracy**:
   This term represents how well the model explains the observed data:

   $$\text{Accuracy} = \frac{1}{2} \pi_o (o - \mu)^2$$

   Where:
   - $o$ is the observed data
   - $\pi_o$ is the likelihood precision

4. **Complete Free Energy equation**:
   Combining the above, we get:

   $$F = \frac{1}{2} \left( \pi_p (\mu - \mu_p)^2 + \ln(\pi_p) - \ln(\sigma) + \frac{\sigma}{\pi_p} - 1 \right) - \frac{1}{2} \pi_o (o - \mu)^2$$

The optimization problem can be expressed as:

$$\arg\min_{\mu, \sigma} F(\mu, \sigma)$$

This formulation captures the essence of free energy minimization in active inference for continuous state spaces, where the agent tries to find the best compromise between its prior beliefs and the observed data.

# Shapley values simulation

Here, as agents achieve more firm and environmental returns, outcomes for the niche are generated. For now, these outcomes are:
- $s^{(2)}_1$ = firm's sustainability indicator
- $s^{(2)}_2$ = environment's sustainability indicator
- agent's own free energy minimization is kept as a continuous value



- Table example: https://www.perplexity.ai/search/say-you-had-5-employees-all-wo-ePh_obDmR4yYeqN42KWdBA

The Shapley value formula:

$$\phi_i(v) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|! (n-|S|-1)!}{n!} (v(S \cup \{i\}) - v(S))$$

This can also be written as:

$$\phi_i(v) = \frac{1}{n} \sum_{S \subseteq N \setminus \{i\}} {n-1 \choose |S|}^{-1} (v(S \cup \{i\}) - v(S))$$

The synergy function $w$ is:

$$w(S) = \begin{cases}
p, & \text{if } S = \{o,w_i\} \\
0, & \text{otherwise}
\end{cases}$$

For subadditive set functions:

If $v$ is a subadditive set function, i.e., $v(S \cup T) \leq v(S) + v(T)$, then for each agent $i$: $\phi_i(v) \leq v(\{i\})$.

For superadditive set functions:

If $v$ is a superadditive set function, i.e., $v(S \cup T) \geq v(S) + v(T)$, then for each agent $i$: $\phi_i(v) \geq v(\{i\})$.

### Example calculation
- To determine the marginal contribution of $A$ out of $agent_i \in \{A,B,C,D,E\}$
$$
\begin{array}{|c|c|c|c|c|c|}
\hline
\text{Coalition (S)} & v(S) & v(S \cup \{A\}) & \text{Marginal Contribution} & \text{Weight} & \text{Weighted Contribution} \\
\hline
\{\} & 0 & 30 & 30 & 1/5 & 6 \\
\{B\} & 20 & 60 & 40 & 1/20 & 2 \\
\{C\} & 15 & 55 & 40 & 1/20 & 2 \\
\{D\} & 10 & 50 & 40 & 1/20 & 2 \\
\{E\} & 5 & 45 & 40 & 1/20 & 2 \\
\{B,C\} & 40 & 80 & 40 & 1/30 & 1.33 \\
\{B,D\} & 35 & 75 & 40 & 1/30 & 1.33 \\
\{B,E\} & 30 & 70 & 40 & 1/30 & 1.33 \\
\{C,D\} & 30 & 70 & 40 & 1/30 & 1.33 \\
\{C,E\} & 25 & 65 & 40 & 1/30 & 1.33 \\
\{D,E\} & 20 & 60 & 40 & 1/30 & 1.33 \\
\{B,C,D\} & 55 & 90 & 35 & 1/20 & 1.75 \\
\{B,C,E\} & 50 & 85 & 35 & 1/20 & 1.75 \\
\{B,D,E\} & 45 & 80 & 35 & 1/20 & 1.75 \\
\{C,D,E\} & 40 & 75 & 35 & 1/20 & 1.75 \\
\{B,C,D,E\} & 70 & 100 & 30 & 1/5 & 6 \\
\hline
\end{array}
$$


Sum of weighted contributions: 30
Interpretation
In this example, A's Shapley value is 30, which represents 30% of the total value (100) created by the team. This means that, on average, A's marginal contribution to any coalition is 30% of the total value.
"""

import random
import numpy as np
import itertools

class Agent:
    def __init__(self, name, role):
        self.name = name
        self.role = role
        self.state = random.randint(0, 2)  # 3 possible states
        self.uncertainty = random.uniform(0.5, 1.0)
        self.compensation = 0
        self.actions = self.get_actions()

    def get_actions(self):
        if self.role == "Manager":
            return ["Plan", "Delegate", "Review", "Motivate", "Report"]
        elif self.role == "Developer":
            return ["Code", "Debug", "Refactor", "Test", "Document"]
        elif self.role == "Designer":
            return ["Sketch", "Prototype", "User Test", "Iterate", "Present"]
        elif self.role == "Analyst":
            return ["Collect Data", "Analyze", "Model", "Visualize", "Recommend"]
        elif self.role == "Tester":
            return ["Write Tests", "Execute Tests", "Report Bugs", "Verify Fixes", "Automate"]

    def choose_action(self):
        return random.choice(self.actions)

    def minimize_uncertainty(self, action):
        action_effect = random.uniform(0.05, 0.2)
        state_modifier = 1 + (self.state * 0.1)
        role_modifier = {"Manager": 0.9, "Developer": 1.1, "Designer": 1.0, "Analyst": 1.2, "Tester": 1.0}[self.role]

        uncertainty_reduction = action_effect * state_modifier * role_modifier
        self.uncertainty = max(0, self.uncertainty - uncertainty_reduction)

        # Update state
        self.state = (self.state + 1) % 3

        return uncertainty_reduction

    def calculate_free_energy(self, uncertainty_reduction):
        task_performance = random.uniform(0.5, 1.0)
        return uncertainty_reduction + task_performance

def calculate_shapley_values(agents, timestep_results):
    num_agents = len(agents)
    shapley_values = {agent.name: 0 for agent in agents}

    free_energy_results = [timestep_results[agent.name]['free_energy'] for agent in agents]
    total_free_energy = sum(free_energy_results)

    if total_free_energy == 0:
        return shapley_values  # Avoid division by zero

    # Calculate contributions for all permutations of agents
    for perm in itertools.permutations(agents):
        for i, agent in enumerate(perm):
            # Calculate the marginal contribution of the agent
            subset = perm[:i]  # Subset of agents before current agent
            subset_free_energy = sum(timestep_results[a.name]['free_energy'] for a in subset)
            current_free_energy = total_free_energy
            marginal_contribution = current_free_energy - subset_free_energy

            # Calculate the Shapley value
            shapley_values[agent.name] += marginal_contribution / (num_agents * len(list(itertools.permutations(agents))))

    return shapley_values

class Firm:
    def __init__(self, agents):
        self.agents = agents
        self.total_compensation = 0

    def run_simulation(self, timesteps):
        results = []
        for t in range(timesteps):
            timestep_results = {"timestep": t}
            total_free_energy = 0

            for agent in self.agents:
                action = agent.choose_action()
                uncertainty_reduction = agent.minimize_uncertainty(action)
                free_energy = agent.calculate_free_energy(uncertainty_reduction)
                total_free_energy += free_energy

                timestep_results[agent.name] = {
                    "action": action,
                    "uncertainty": agent.uncertainty,
                    "free_energy": free_energy
                }

            # Calculate Shapley values for this timestep
            shapley_values = calculate_shapley_values(self.agents, timestep_results)

            # Calculate compensation based on Shapley values
            total_shapley = sum(shapley_values.values())
            for agent in self.agents:
                agent.compensation = (shapley_values[agent.name] / total_shapley) * 5  # Assuming total compensation of 5 units
                self.total_compensation += agent.compensation
                timestep_results[agent.name]["compensation"] = agent.compensation

            results.append(timestep_results)

        return results

# Create agents
agents = [
    Agent("Agent1", "Manager"),
    Agent("Agent2", "Developer"),
    Agent("Agent3", "Designer"),
    Agent("Agent4", "Analyst"),
    Agent("Agent5", "Tester")
]

# Create firm and run simulation
firm = Firm(agents)
simulation_results = firm.run_simulation(100)
print(simulation_results)
print(type(simulation_results))

import pandas as pd

# Flatten the data
data = [
    {
        'timestep': d['timestep'],
        'agent': agent,
        'action': d[agent]['action'],
        'uncertainty': d[agent]['uncertainty'],
        'free_energy': d[agent]['free_energy'],
        'compensation' : d[agent]['compensation']
    }
    for d in simulation_results
    for agent in ['Agent1', 'Agent2', 'Agent3', 'Agent4', 'Agent5']
]

# Create the DataFrame
df = pd.DataFrame(data)

# Set the index to be a MultiIndex of timestep and agent
#df = df.set_index(['timestep', 'agent'])
# Sort the index
#df = df.sort_index()
print(df.to_dict())
display(df)

# Print results
for result in simulation_results:
    print(f"\nTimestep {result['timestep']}:")
    for agent_name, agent_data in result.items():
        if agent_name != "timestep":
            print(f"  {agent_name}: Action: {agent_data['action']}, Uncertainty: {agent_data['uncertainty']:.4f}, Free Energy: {agent_data['free_energy']:.4f}, Compensation: {agent_data['compensation']:.4f}")

print(f"\nTotal Firm Compensation: {firm.total_compensation:.4f}")

import matplotlib.pyplot as plt
import pandas as pd

# Assuming df is your DataFrame
# If not, you'll need to load your data into df first

plt.figure(figsize=(12, 8))

agents = ['Agent1', 'Agent2', 'Agent3', 'Agent4', 'Agent5']
colors = ['blue', 'red', 'green', 'magenta', 'cyan']
line_styles = ['-', '--',':']

for i, agent in enumerate(agents):
    agent_data = df[df['agent'] == agent]

    # Plot uncertainty
    plt.plot(agent_data['timestep'], agent_data['uncertainty'],
             color=colors[i], linestyle=line_styles[0],
             marker='o', label=f'{agent} - Uncertainty')
    # Plot free energy
    plt.plot(agent_data['timestep'], agent_data['free_energy'],
             color=colors[i], linestyle=line_styles[1],
             marker='x', label=f'{agent} - Free Energy')
    # Plot free energy
    plt.plot(agent_data['timestep'], agent_data['compensation'],
             color=colors[i], linestyle=line_styles[2],
             marker='x', label=f'{agent} - Free Energy')

plt.xlabel('Timestep')
plt.ylabel('Value')
plt.title('Uncertainty and Free Energy Over Time for Each Agent')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Assuming df is your DataFrame
# If not, you'll need to load your data into df first

agents = ['Agent1', 'Agent2', 'Agent3', 'Agent4', 'Agent5']
fig, axs = plt.subplots(5, 1, figsize=(12, 16), sharex=True)
fig.suptitle('Agent Metrics Over Time', fontsize=16)

# Create a color map for actions
unique_actions = df['action'].unique()
color_map = plt.cm.get_cmap('tab20')
action_colors = {action: color_map(i/len(unique_actions)) for i, action in enumerate(unique_actions)}

for i, agent in enumerate(agents):
    agent_data = df[df['agent'] == agent]

    # Plot free energy, uncertainty, and compensation
    axs[i].plot(agent_data['timestep'], agent_data['free_energy'], 'b-', label='Free Energy')
    axs[i].plot(agent_data['timestep'], agent_data['uncertainty'], 'r-', label='Uncertainty')
    axs[i].plot(agent_data['timestep'], agent_data['compensation'], 'g-', label='Compensation')

    # Add action dots
    for _, row in agent_data.iterrows():
        axs[i].scatter(row['timestep'], -0.1, color=action_colors[row['action']], s=100, zorder=3)
        axs[i].annotate(row['action'], (row['timestep'], -0.15), ha='center', va='top', rotation=90, fontsize=8)

    axs[i].set_title(agent)
    axs[i].set_ylabel('Value')
    axs[i].legend(loc='upper right')
    axs[i].grid(True, linestyle='--', alpha=0.7)
    axs[i].set_ylim(-0.2, 1.5)  # Adjust as needed

# Set common x label
fig.text(0.5, 0.04, 'Timestep', ha='center', va='center')

# Add a legend for actions
action_legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label=action,
                          markerfacecolor=color, markersize=10) for action, color in action_colors.items()]
fig.legend(handles=action_legend_elements, loc='lower center', ncol=5, title='Actions')

plt.tight_layout()
plt.subplots_adjust(bottom=0.1)
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# Assuming df is your DataFrame
# If not, you'll need to load your data into df first

# Calculate the average metrics per timestep
avg_metrics = df.groupby('timestep').agg({
    'free_energy': 'mean',
    'compensation': 'mean',
    'uncertainty': 'mean'
}).reset_index()

# Create the plot
plt.figure(figsize=(12, 6))

# Plot each metric
plt.plot(avg_metrics['timestep'], avg_metrics['free_energy'], 'b-o', label='Avg Free Energy')
plt.plot(avg_metrics['timestep'], avg_metrics['compensation'], 'g-s', label='Avg Compensation')
#plt.plot(avg_metrics['timestep'], avg_metrics['uncertainty'], 'r-^', label='Avg Uncertainty')

# Customize the plot
plt.xlabel('Timestep')
plt.ylabel('Average Value')
plt.title('Average Metrics Across All Agents Over Time')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.7)

# Set x-axis ticks to integer values
plt.xticks(avg_metrics['timestep'])

# Add value labels on the points
for column in ['free_energy', 'compensation']:  #['free_energy', 'compensation', 'uncertainty']:
    for x, y in zip(avg_metrics['timestep'], avg_metrics[column]):
        plt.annotate(f'{y:.2f}', (x, y), textcoords="offset points", xytext=(0,10), ha='center')

plt.tight_layout()
plt.show()

pip install nbconvert wkhtmltopdf

npath = './ActInf_BioFirm_v_01.ipynb'
opath = './'

!pip install nbconvert

import nbconvert
import os
from traitlets.config import Config
from nbconvert.preprocessors import Preprocessor

class CodeBlockPreprocessor(Preprocessor):
    def preprocess_cell(self, cell, resources, index):
        if cell['cell_type'] == 'code':
            cell['source'] = f"```python\n{cell['source']}\n```"
        return cell, resources

# Define custom CSS for code blocks
custom_css = """
<style>
    pre {
        background-color: #f5f5f5;
        border: 1px solid #ccc;
        border-radius: 4px;
        padding: 10px;
        font-family: monospace;
        font-size: 14px;
        line-height: 1.4;
        overflow-x: auto;
    }
</style>
"""

# Configure nbconvert
c = Config()
c.HTMLExporter.preprocessors = [CodeBlockPreprocessor]
c.HTMLExporter.exclude_input_prompt = True
c.HTMLExporter.exclude_output_prompt = True

# Create HTML exporter with custom config
html_exporter = nbconvert.HTMLExporter(config=c)

# Add custom CSS to the template
html_exporter.template_name = 'classic'
html_exporter.template.extra_css = custom_css

# Convert notebook to HTML
notebook_path = npath # Update this path
output_path = opath

(body, resources) = html_exporter.from_filename(notebook_path)

# Save HTML file
html_filename = os.path.join(output_path, 'notebook.html')
with open(html_filename, 'w', encoding='utf-8') as f:
    f.write(body)

print(f"Conversion complete. HTML saved at: {html_filename}")

# Optional: Convert HTML to PDF using Google Chrome's print function
from google.colab import files
files.download(html_filename)

from openai import OpenAI
import numpy as np
import pandas as pd


pro_forma = """Business Model: Innovative Solutions Inc.
Mission Statement
To revolutionize [industry] through cutting-edge technology and ethical practices, while maximizing stakeholder value and societal impact.
Organizational Structure
Our company operates with a committee-based structure to ensure efficient decision-making and specialized focus across key areas:
Executive Committee
Research & Development (R&D) Committee
Ethics & Safety Committee
Financial Committee
Marketing Committee
Staff Allocation
Each employee contributes to the operations of their respective committee, fostering expertise and streamlined processes.
Committee Responsibilities
Executive Committee
Strategic planning and goal-setting
Overall company management and leadership
Inter-committee coordination and resource allocation
R&D Committee
Product innovation and development
Technology research and implementation
Quality assurance and continuous improvement
Ethics & Safety Committee
Ethical guidelines development and enforcement
Safety protocols and risk management
Compliance with industry regulations and standards
Financial Committee
Budgeting and financial planning
Investment strategies and capital allocation
Financial reporting and analysis
Marketing Committee
Brand development and management
Market research and customer insights
Sales strategies and customer acquisition
Operational Model
Ideation and Research: R&D Committee generates innovative concepts.
Ethical Review: Ethics & Safety Committee assesses potential impacts.
Financial Feasibility: Financial Committee evaluates economic viability.
Development: R&D Committee creates prototypes or services.
Marketing Strategy: Marketing Committee develops go-to-market plans.
Executive Approval: Executive Committee provides final authorization.
Launch and Monitoring: All committees collaborate on implementation and performance tracking.
Key Performance Indicators (KPIs)
Revenue growth
Customer satisfaction scores
Innovation index (new products/services per year)
Ethical compliance rate
Return on investment (ROI)
Market share
Funding and Revenue Model
Initial funding through [venture capital/angel investors/bootstrapping]
Revenue streams: [product sales/service subscriptions/licensing]
Profit sharing model among stakeholders
Growth Strategy
Expand product/service offerings
Enter new markets and geographic regions
Form strategic partnerships and collaborations
Continuous talent acquisition and development
Risk Management
Regular risk assessments by Ethics & Safety Committee
Diversification of product/service portfolio
Robust cybersecurity measures
Ongoing market trend analysis"""


roles = ['executive','r&d','ethics','financial','marketing']

import openai

YOUR_API_KEY = "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"
client = openai.OpenAI(api_key=YOUR_API_KEY)

# List of prompts to send to the LLM
prompts = [
    "Based on the provided pro forma business model for Innovative Solutions Inc., please elaborate on the specific responsibilities, processes, and interactions for each of the five committees (Executive, R&D, Ethics & Safety, Financial, and Marketing). Include details on how these committees would collaborate and make decisions in a real-world scenario.",
    "For each of the five committees (Executive, R&D, Ethics & Safety, Financial, and Marketing) in our business model, generate 5 distinct actions that an AI agent representing that committee could take. These actions should be in the form of prompts that the agent can send to an LLM to accomplish its tasks. Ensure that these actions are specific, relevant to the committee's responsibilities, and diverse in their approach to problem-solving.",
    "Given that each agent receives 5 observations per timestep (4 from other agents and 1 from itself), where the observations from other agents are integers 1 (poor), 2 (moderate), or 3 (good), and the self-observation is the index of the last action taken (0-4), provide a framework for interpreting these observations. How should an agent weigh these inputs to infer the quality of its own work?",
    "Describe a method for an agent to infer its hidden state factor 'quality' (as a categorical probability distribution of 'low' and 'high') based on the observations it receives. How should the agent update its belief about its work quality given the feedback from other agents and its own last action?",
    "Given an agent's current belief about its work quality (hidden state) and the set of 5 possible actions, propose a decision-making process for the agent to select its next action. How should the agent balance exploration (trying new actions) with exploitation (choosing actions that have worked well in the past)?",
    """For a single timestep in the simulation, describe how all five agents should:
a) Receive and interpret observations
b) Update their beliefs about their work quality
c) Select and execute their next action
d) Generate observations for other agents based on their action
Provide a step-by-step process that can be repeated for each timestep of the simulation.""",
    "Performance Evaluation Prompt"
]

# Function to send a prompt to the LLM and get a response
def send_prompt_to_llm(prompt, industry):
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
        {"role": "system", "content": f"You are a skilled business professional in the industry of {industry}"},
        {"role": "user", "content": f"""{prompt}
         """}
          ],
            max_tokens=150,
            n=1,
            stop=None,
            temperature=0.7,
        )
        return response.choices[0].text.strip()
    except Exception as e:
        return f"Error: {str(e)}"

def send_prompt_to_llm(prompt,industry):
  # Send the request to the GPT-4o mini model based on which agent
  response = client.chat.completions.create(
      model="gpt-4o-mini",
      messages=[
        {"role": "system", "content": f"You are a skilled business professional in the industry of {industry}"},
        {"role": "user", "content": f"""{prompt}"""}
      ],
      temperature=0.7,  # controls randomness vs. coherence
      seed = 1
  )
  return str(response.choices[0].message.content)

# Iteratively send prompts and store responses

industry = "agriculture"

responses = {}
for prompt in prompts:
    print(f"Sending prompt: {prompt}")
    response = send_prompt_to_llm(prompt=prompt, industry=industry)
    responses[prompt] = response
    print(f"Response received: {response[:50]}...")  # Print first 50 characters
    print()

# Print all responses
print("All responses:")
for prompt, response in responses.items():
    print(f"\n{prompt}:")
    print(response)

print(responses)


print(list(responses.keys()))
f_text = ""
for i in range(len(responses)):
  f_text = f_text + "\n \n" + list(responses.keys())[i] + "\n \n" + responses[list(responses.keys())[i]]

!pip install tiktoken
import tiktoken


def count_tokens(text):
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    return len(encoding.encode(text))

token_count = count_tokens(f_text)
print(f"Total number of tokens in the combined file: {token_count}")


!pip install fpdf

# Export full_text to pdf
from fpdf import FPDF
import textwrap

class PDF(FPDF):
    def __init__(self):
        super().__init__()
        self.set_auto_page_break(auto=True, margin=15)

    def chapter_body(self, body):
        self.set_font("Times", size=12) #"B", size=12)
        self.set_left_margin(10)
        self.set_right_margin(10)

        paragraphs = body.split('\n')
        for paragraph in paragraphs:
            lines = textwrap.wrap(paragraph, width=80)  # Adjust width as needed
            if lines:
                for line in lines:
                    self.cell(0, 5, txt=line.encode('latin-1', 'replace').decode('latin-1'), ln=1)
            else:
                self.ln()  # Empty line

def create_pdf_from_string(text, output_filename):
    pdf = PDF()
    pdf.add_page()
    pdf.chapter_body(text)
    pdf.output(output_filename, 'F')

# Create the PDF
create_pdf_from_string(f_text, "f_text.pdf")

print(responses[])

"""# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
"""

roles =

"""### Experiment 1:
- single 'literature review' agent with `role` assigned and `field`, `phenomena`, and `prompt` variables.
- sends single `role` and `prompt` message to agent
"""

from openai import OpenAI
import numpy as np
import pandas as pd

# Initialize the OpenAI client
YOUR_API_KEY = YOUR_API_KEY
client = OpenAI(api_key=YOUR_API_KEY)

# Define your prompt
field = "Active Inference"
phenomena = "the cognitive aspects of PTSD and therapeutic treatment interventions"


# Define role
role = "literature"
role_prompt = f"You are an expert in the {role} on {field} and are working on a project on applying the methods of {field} to the study of {phenomena}."
prompt = """Write a brief overview of the {role} {field} and its potential uses for the study of {phenomena}. Include citations to the references you use. You are only working on the
              literature section so do not include any additional comments beyond the literature review.
            At the very end of your response, grade how well your review conforms to the literature on a scale of 1 to 3 enclosed in double brackets, for example [[1]] to mean 'poorly'."""
# Send the request to the GPT-4o mini model
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": role_prompt},
        {"role": "user", "content": prompt}
    ],
    temperature=0.0  # controls randomness vs. coherence
)

# Print the response
print(response.choices[0].message.content)

# Extract score
import re
match = re.search(r'\[\[(\d+)\]\]', str(response.choices[0].message.content))
number = match.group(1) if match else None
print(match)
print(f"Extract score: {number}")

# Review LLM response `Choice` object attributes and contents
print(response.choices)

"""# Experiment 2

## Pulling the levers:
- **1. User input**: To start, the user simply defines two strings: the `field` of study and `phenomena` of interest to which the `field` should be applied. For example, the user might desire to apply `field = "Active Inference"` to the study of `phenomena = "human affect"`.
- **2. LLM 'roles'**: A series of roles--here, five--are defined and sent as components of prompts to the OpenAI `chat-gpt-4o-mini` model. A 'role' is at once a prompt to tell the LLM 'who they are' and what they are working on; and also representative of a section of a paper, e.g., a literature review section, a methods section, a novelty section, a safety/ethics section, an abstract.

- **3. Initial phase**: each role is contacted (an API call is made) to create their respective section of the paper, as guided by the `field` and `phenomena` fields. At the end of their responses containing their section, the LLMs also include a self-grade of their section enclosed in brackets. The grades are `[[1]]` (poor quality), `[[2]]` (medium quality), and `[[3]]` (high quality). The brackets are requested to simplify the ability to extract these scores from the text as individual integers. These scores are recorded and encoded into an agent-readable vector of observations, `obs`. There is an additional element added to the end of `obs` representing what action the agent has taken; since they have done nothing so far, it will represent `NULL`.
  - This current version employs a rudimentary self-grade system for demonstration purposes. LLM's self-observations of their own work, unsurprisingly, are quite confident with `3` being the typical output.
  - Each section is then collected and stored, individually as well as in a concatenated string entitled `full_text`, in a Python dictionary called `project`. More specifically, this is all stored in `project[0]`--the first element in the dictionary--where each element represents a timestep. Therefore `project[0]` contains the materials of the project at the `0`th timestep.
- **4. Action phase**: For each subsequent timestep, the agent may choose which section of the paper to update next (if any, as there is also a `NULL` action the agent can choose to make no changes during that timestep). All materials are once again recorded--more specifically, they are copied over from the prior timestep and then the agent will choose to update one (or none) of them--and the final state of progress is once again stored in the dictionary, in `project[1]`.
- **5. Check phase**: There is a final 'check' phase where LLMs are again contacted to re-grade their sections, now prompted with the `full_text` as well as their own section (to be clear on which part of the `full_text` they should modify).
- [**Perception-action loop**: Repeat steps **4** and **5** for `T` timesteps]
- **6. Finalize**: A final call to an LLM is made to come up with a title, which is placed at the top during a re-ordering of the paper. The paper is then formatted into a .pdf document and stored locally.


## Agent construction:
- Building the agent: our agent infers the quality of its paper over time as a single internal hidden state factor, $s^{quality}$, whose discrete levels are `low` and `high`. The agent takes actions to attempt to control this hidden state, to move from a `low` to `high` belief, via receiving observations from the LLM representing evaluations of the work.
  - While the agent will likely benefit from being able to infer various hidden state factors beyond than this one alone, this simplistic construction is sufficient to highlight the ability of a two-level factor to act as a kind of sliding scale, or spectrum, in this case representing the (inferred) quality of the paper the agent is acting to produce.
- $\boxed{\boldsymbol{o}}$ : one observation modality per LLM role, with levels `0,1,2` corresponding to `low, medium, high` quality scores (ex. agent can observe 'low' quality in literature section, 'medium' quality in methods section, etc.). Agent also has one observation modality representing its last action taken so that it can observe its own behavior.
    - ex. if agent receives observation [0,1,1,1,2,5], then this means "first section of paper received 'poor' score (`0`), next three sections received 'medium' scores (`1,1,1`), last section of paper received high score (`2`), agent did nothing in last timestep (`5`)
    - `obs` will be structured as the self-graded quality scores as well as last action taken, $[o_{literature},o_{novelty}, o_{safety},o_{methods}, o_{synthesizer},o_{\pi_{t-1}}]$
      - the `synthesizer` role constructs the abstract post-hoc, via a prompt asking it to synthesize the existing sections
- $\boxed{\boldsymbol{s}}$ : single hidden state factor representing paper quality as `'low'` or `'high'`.
    - the simplicity of this particular hidden state factor is that we can look at the grade as on an ordered, sliding scale rather than as unordered categorical determinations.
- $\boxed{\boldsymbol{\pi}}$ : agent has one action per section in the paper, e.g., 'literature' to revise literature section. Agent additionally has a `'NULL'` action which does nothing for the timestep it is committed.
- $\boxed{\boldsymbol{A}}$: $\text{likelihood model} \hspace{5mm} P(o|s)$
    - For the section-related modalities, agent makes the following observation-state associations:
      - `low` score $o_{low}$ with `high` quality paper $s_{high}$ with probability `0.0`
      - `medium` score $o_{medium}$ with `high` quality paper $s_{high}$ with probability `0.25`
      - `high` score $o_{high}$ with `high` quality paper $s_{high}$ with probability `0.75`
      - values chosen reflect scaling (and obvious requirement of summing to 1). Agents should not act to avoid, say, raising a `low` score to a `medium` score.
    - For `last_action_taken` modality, agent is initially entirely uncertain which observed self-action is associated with $s\in \{low,high\}$
    - **Agent learns** $\boxed{\boldsymbol{A}}$ over time
- $\boxed{\boldsymbol{B}}$: $\text{transition model} \hspace{5mm} P(s_{t+1}|s_t,\pi)$

    - Agent begins with full uncertainty as to which actions $\pi$ lead from $s_t\in \{low,high\}$ to $s_{t+1}\in \{low,high\}$
    - **Agent learns** $\boxed{\boldsymbol{B}}$ over time
  - $\boxed{\boldsymbol{c}}$ : $\text{preferences} \hspace{5mm} P(o)$
    - Agent's 'preferences' it attempts to realize via their incorporation into EFE
    - for section-related modalities: equivalent to their values in $\boldsymbol{A}$, i.e. agent prefers $P(o_{low})=0.0$, $P(o_{medium})=0.25$, $P(o_{high})=0.75$ i.e. `[0.0, 0.25, 0.75]`
    - for self-observed action modality: uniform for all paper section-related actions and zero for NULL, i.e. agent doesn't prefer (seeing itself) revising any section in particular more than the other sections, but it also does not prefer (seeing itself) doing nothing.
      - For example, if available actions are `[revise_literature, revise_novelty, revise_safety, revise_methods, revise_abstract, NULL]` then preference probabilities should be `[0.2,0.2,0.2,0.2,0.2,0.0]`
- $\boxed{\boldsymbol{d}}$ : $\text{priors over initial states} \hspace{5mm} P(s_0)$
    - initialized as $[1.0,0.0]$, i.e. the agent assumes at onset that it has a very poor quality paper.
    - **Agent learns** $\boxed{\boldsymbol{d}}$ over time
- $\boxed{\boldsymbol{e}}$ : $\text{habits} \hspace{5mm} P(\pi_0)$
    - initialized as fully uniform, i.e. agent has a 'habit' of all actions--including the `NULL` action--equally and these habits are *not learned over time*.
    - This effectively means habits have no impact on policy inference $Q(\pi) = \sigma(-G_\pi*\gamma -F_\pi - \ln{E})$; they will not contribute to weighting any particular action differently from the others.


## Initial phase:
- Define:
  - roles and initial prompts per LLM 'role' which take as inputs:
    - information regarding roles
    - the `field` and `phenomena` which direct the aim of the project
    - outputs the section along with a 'self-grade' as to the quality of the section.
      - **note:** the self-grade here is not being taken into account, as the LLM is frequently rather proud of its work from the start and at this step lacks information about the other sections being constructed
  - universal action prompt which takes the chosen section to modify as an input and generates a prompt for that section's LLM role to revise that section, based on the corresponding role

- Process:
  - run all initial prompts, store individual sections in a `project` dict for timestep `0`

## Action-perception loop:
- Process:
  - Initialize setup:
    - copy over full project from initial phase (timestep `0`)
    - Initialize agent
    - force first action (stored in `action_sampled_id`) to be the last-indexed available action, which will be the `NULL` action. This begins the loop.
  - Agent receives `obs` (section scores) extracted from initial phase to infer $$Q(s)$$.
  - Agent learns (updates parameters of) its $\boldsymbol{A}$ likelihood matrix.
  - Agent infers $Q(\pi)$ and samples (chooses) an action from this distribution.
  - Agent learns (updates parameters of) its $\boldsymbol{B}$ state transitions matrix.
  - Agent learns (updates parameters of) its $\boldsymbol{d}$ state transitions matrix.
  - agent's selected action, if a 'role' related action is chosen, triggers sending corresponding prompt to send full paper contents and request to update corresponding section. If selected action is `NULL`, do nothing.
  - A "check phase" re-grades the paper and extracts the scores to generate, along with the `action_sampled_id`, the `obs` for the next timestep
    - This particular sub-process can be optimized or reconceptualized.


"""

from graphviz import Digraph
from IPython.display import Image, display

def create_experiment_flowchart():
    dot = Digraph(comment='Experiment 2 Flowchart')
    dot.attr(rankdir='TB', size='12,12')

    # Define nodes
    dot.node('A', 'User Input')
    dot.node('B', 'Define LLM Roles')
    dot.node('C', 'Initial Phase')
    dot.node('D', 'Store Results')
    dot.node('E', 'Action Phase')
    dot.node('F', 'Check Phase')
    dot.node('G', 'Repeat')
    dot.node('H', 'Finalize')

    # Define edges
    dot.edge('A', 'B', 'Field & Phenomena')
    dot.edge('B', 'C')
    dot.edge('C', 'D', 'Initial Sections')
    dot.edge('D', 'E')
    dot.edge('E', 'F', 'Update Section')
    dot.edge('F', 'G', 'Re-grade')
    dot.edge('G', 'E', 'Next Timestep')
    dot.edge('G', 'H', 'After T Timesteps')

    # Subgraph for Agent Construction
    with dot.subgraph(name='cluster_0') as c:
        c.attr(label='Agent Construction')
        c.node('AC1', 'Hidden State s')
        c.node('AC2', 'Observations o')
        c.node('AC3', 'Actions π')
        c.node('AC4', 'Likelihood Model A')
        c.node('AC5', 'Transition Model B')
        c.node('AC6', 'Preferences c')
        c.node('AC7', 'Initial State Priors d')
        c.node('AC8', 'Habits e')

    # Connect Agent Construction to main flow
    dot.edge('AC1', 'E')
    dot.edge('AC2', 'E')
    dot.edge('AC3', 'E')

    # Render the graph
    dot.render('experiment_flowchart', format='png', cleanup=True)
    print("Flowchart image generated as 'experiment_flowchart.png'")

    # Display the image in the console
    display(Image(filename='experiment_flowchart.png'))

# You can now directly call this function in your Jupyter notebook
create_experiment_flowchart()

from graphviz import Digraph
from IPython.display import Image, display

# Create a new directed graph
dot = Digraph(comment='Scientific Paper Writer Simulation')
dot.attr(rankdir='TB', size='16,16', ratio='compress', fontname='Courier', fontsize='14')

# Define node styles
dot.attr('node', shape='box', style='filled', color='lightblue', margin='0.2', fontname='Courier', fontsize='14')
dot.attr('edge', penwidth='1.5', fontname='Courier', fontsize='14')

# Main process nodes
dot.node('1', '1. User Input\nField & Phenomena')
dot.node('2', '2. Initialize LLM Roles')
dot.node('3', '3. Initial Phase\nGenerate Sections')
dot.node('6', '6. Finalize Paper')

# Create a subgraph for the Perception-Action Loop
with dot.subgraph(name='cluster_pa') as pa:
    pa.attr(label='<<b>Perception-Action Loop</b>>', style='filled', color='lightgreen', fontname='Courier', fontsize='16')
    pa.attr(rankdir='LR')
    pa.node('4', '4. Action Phase\nRevise Section or NULL')
    pa.node('5', '5. Check Phase\nRe-grade Paper')

# Agent components
with dot.subgraph(name='cluster_agent') as c:
    c.attr(label='<<b>Active Inference Agent</b>>', style='filled', color='pink', fontname='Courier', fontsize='16')
    c.attr(rankdir='TB')
    c.node('S', 'Hidden State (s)\nPaper Quality: Low/High')
    c.node('O', 'Observations (o)\nSection Scores & Last Action')
    c.node('Pi', 'Actions (π)\nRevise Sections or NULL')
    c.node('A', 'Likelihood Matrix (A)')
    c.node('B', 'Transition Matrix (B)')
    c.node('C', 'Preferences (c)')
    c.node('D', 'Prior over Initial States (d)')
    c.node('E', 'Habits (e)')

# Action Prompts node
dot.node('P', 'Action Prompts\n(Literature, Novelty, Safety, Methods, Synthesizer)')

# Add main process edges
dot.edge('1', '2')
dot.edge('2', '3')
dot.edge('3', '4', constraint='false')
dot.edge('5', '6')

# Connect Perception-Action Loop
dot.edge('4', '5')
dot.edge('5', '4', label='Repeat for T timesteps')

# Connect Agent components
dot.edge('S', 'A', dir='both')
dot.edge('O', 'A', dir='both')
dot.edge('S', 'B', dir='both')
dot.edge('Pi', 'B', dir='both')
dot.edge('O', 'C')
dot.edge('S', 'D', dir='both')
dot.edge('Pi', 'E')

# Connect Agent to main process
dot.edge('5', 'O', label='Provides\nobservations', style='dashed', color='grey')
dot.edge('3', 'O', label='Initial\nobservations', style='dashed', color='grey', constraint='false')
dot.edge('Pi', '4', label='sample')
dot.edge('P', '4', label='', style = 'dashed', color = 'grey')

# Render the graph
dot.render('experiment_flowchart', format='png', cleanup=True)
print("Graph has been generated as 'experiment_flowchart.png'")

# Display the image in the console
display(Image(filename='experiment_flowchart.png'))

from graphviz import Digraph
from IPython.display import Image, display

# Create a new directed graph
dot = Digraph(comment='Scientific Paper Writer Simulation')
dot.attr(rankdir='LR', size='16,16', ratio='auto', fontname='Courier', fontsize='16')

# Define node styles
dot.attr('node', shape='rectangle', style='filled,rounded', color='#E6F3FF',
         fontname='Courier', fontsize='16', margin='0.2', penwidth='1.5')
dot.attr('edge', penwidth='1', fontname='Courier', fontsize='14', color='#666666')

# Main process nodes
dot.node('1', '1. User Input\n("Field" & "Phenomena")', shape='ellipse', color='#FFD700')
dot.node('2', '2. Initialize LLM\nRoles & Prompts\n(Literature, Novelty, Safety,\nMethods, Synthesizer, NULL)', color='#98FB98')
dot.node('3', '3. Initial Phase\n(Generate Sections &\nInitial grades)', color='#98FB98')
dot.node('6', '6. Finalize Paper', shape='ellipse', color='#FFD700')

# Create a subgraph for the Perception-Action Loop
with dot.subgraph(name='cluster_pa') as pa:
    pa.attr(label='Perception-Action Loop', style='filled', color='#E6FFE6',
            fontname='Courier', fontsize='18', fontcolor='#333333')
    pa.attr(rankdir='LR')
    pa.node('4', '4. Action Phase\n(LLM: Revise Section or NULL)', color='#98FB98')
    pa.node('5', '5. Check Phase\n(LLM: Re-grade Paper)', color='#98FB98')

# Agent components
with dot.subgraph(name='cluster_agent') as c:
    c.attr(label='Active Inference Agent', labeljust='l', labelloc='t',
           style='filled', color='#FFE6E6', fontname='Courier', fontsize='18', fontcolor='#333333')
    c.attr(rankdir='TB')
    c.node('S', 'Hidden State (s)\nPaper Quality: Low/High', color='#FFA07A')
    c.node('A', 'Likelihood Matrix (A)', color='#FFA07A')
    c.node('B', 'Transition Matrix (B)', color='#FFA07A')
    c.node('C', 'Preferences (c)', color='#FFA07A')
    c.node('D', 'Prior over Initial States (d)', color='#FFA07A')
    c.node('E', 'Habits (e)', color='#FFA07A')

    # Subgraph for Observations (O)
    with c.subgraph(name='cluster_O') as co:
        co.attr(label='', labeljust='r', labelloc='t',
                style='filled', color='#E6E6FF', fontname='Courier', fontsize='16', fontcolor='#333333')
        co.node('O', 'Observations (o)\nSection Scores & Last Action', color='#ADD8E6')

    # Subgraph for Actions (Pi)
    with c.subgraph(name='cluster_Pi') as cpi:
        cpi.attr(label='', labeljust='r', labelloc='t',
                 style='filled', color='#FFE6FF', fontname='Courier', fontsize='16', fontcolor='#333333')
        cpi.node('Pi', 'Actions (π)\nRevise Sections or NULL', color='#DDA0DD')

# # Action Prompts node
# dot.node('P', 'LLM Prompts\n(Literature, Novelty, Safety,\nMethods, Synthesizer, NULL)',
#          shape='note', color='#F0E68C')

# Add main process edges
dot.edge('1', '2')
dot.edge('2', '3')
#dot.edge('3', '4', constraint='false')
dot.edge('5', '6',label='After T timesteps', fontcolor = '#333333')

# Connect Perception-Action Loop
dot.edge('4', '5')
dot.edge('5', '4', label='Repeat for\n T timesteps', fontcolor='#333333')

# Connect Agent components
dot.edge('S', 'A', dir='both', color='#FF6347')
dot.edge('O', 'A', dir='both', color='#FF6347')
dot.edge('S', 'B', dir='both', color='#FF6347')
dot.edge('Pi', 'B', dir='both', color='#FF6347')
dot.edge('O', 'C', dir='both', color='#FF6347')
dot.edge('S', 'D', dir='both', color='#FF6347')
dot.edge('Pi', 'E', dir='both', color='#FF6347')

# Connect Agent to main process
dot.edge('5', 'O', penwidth='2', label='Provides\nobservations', style='dashed', color='#1bc70d',fontcolor='#1bc70d')
dot.edge('3', 'O', penwidth='2', label='\nInitial observations', style='dashed', color='#1bc70d', constraint='false',fontcolor='#1bc70d')
dot.edge('Pi', '4', penwidth='2', label='Sample\n action', style='dashed', color='#b30dc7',fontcolor='#b30dc7')
#dot.edge('P', 'O', style='dashed', color = '#999999')


# Render the graph
dot.render('experiment_flowchart', format='png', cleanup=False)
print("Graph has been generated as 'experiment_flowchart.png'")

# Display the image in the console (if running in a Jupyter notebook)
display(Image(filename='experiment_flowchart.png'))

# Experiment 2: Building the agent
import pymdp
from pymdp import utils
from pymdp.maths import softmax
from pymdp.agent import Agent
import math
import random
np.random.seed(0)
random.seed(0)
import copy

# observations
literature_obs_names = ['poor', 'medium', 'high']
novelty_obs_names = copy.deepcopy(literature_obs_names)
safety_obs_names = copy.deepcopy(literature_obs_names)
methods_obs_names = copy.deepcopy(literature_obs_names)
synthesizer_obs_names = copy.deepcopy(literature_obs_names)
last_action_taken_obs_names = ['literature','novelty','safety','methods','synthesizer','NULL']
num_obs = [len(literature_obs_names), len(novelty_obs_names), len(safety_obs_names),
           len(methods_obs_names), len(synthesizer_obs_names)]
num_modalities = len(num_obs)


# states
quality_state_names = ['low','high']  # with only 2, this sort of functions like a continuous variable...though here nor there, it's still Cat.
num_states = [len(quality_state_names)]
num_factors = len(num_states)
# !!!!!!!!! consider adding self-action state (would have precise B transitions and vicariously have alignment with last_action_taken_obs + learn P(section_improved|action_state) association?)

# actions
action_names = copy.deepcopy(last_action_taken_obs_names)
num_controls = len(action_names)

# LIKELIHOOD BELIEFS P(o|s) -- A: deterministic for all 5 LLM-related modalities, e.g., where P(o[0,1,2]|s=high) = [0.0, 0.25, 0.75) and vice-versa for s=low
A = utils.obj_array(num_modalities)
for i in range(num_modalities-1):
  A[i] = np.zeros( ( (len(literature_obs_names)), len(quality_state_names) ) )   # all modalities (except action-related) have the same number of levels, thus can reuse `literature_obs_names`
  A[i][:,0] = utils.norm_dist(np.array([0.75, 0.25, 0.0]))  #P(o=poor|s=low)=0.75, etc.
  A[i][:,1] = utils.norm_dist(np.array([0.0, 0.25, 0.75]))  #P(o=poor|s=high)=0.0, etc.
# final modality (all 6 actions): agent uncertain how observed actions align with states
A[num_modalities-1] = np.zeros( ( len(last_action_taken_obs_names), len(quality_state_names) ) )
for quality_state in range(len(quality_state_names)):
  A[num_modalities-1][:,quality_state] = utils.norm_dist(np.ones(shape=len(last_action_taken_obs_names)))
print(f"utils.is_normalized(A) = {utils.is_normalized(A)}")  # probabilities must sum vertically; row = observation, column = state

# TRANSITION BELIEFS P(s_{t+1}|s_t,pi) -- B: fully uniform, learned over time
B = utils.obj_array(num_factors)
B[0] = np.zeros( ( len(quality_state_names), len(quality_state_names), len(action_names) ) )
#print(B[0])
# The probability distribution of transitioning to a particular state next...
for quality_state_next in range(len(quality_state_names)):
  # ...given a particular current state...
  for quality_state_current in range(len(quality_state_names)):
    # ...is a uniform (full uncertainty) distribution over all actions
    B[0][quality_state_next,quality_state_current,:] = utils.norm_dist(np.ones(shape=len(action_names)))
    #print(utils.is_normalized(B[0][quality_state_next,quality_state_current,:]))
B[0] = utils.norm_dist(B[0])
print(f"utils.is_normalized(B) = {utils.is_normalized(B)}")
#print(f"utils.is_normalized(B[0]) = {utils.is_normalized(B[0])}")

# PREFERENCES P(o) -- C: similar to A in terms of how much each score is preferred
C = utils.obj_array_zeros(num_obs)
# For each modality (except action modality)...
for modality in range(num_modalities-1):
  # ... P(o=low) = 0.0, P(o=medium) = 0.25, P(o=high) = 0.75
  C[modality] = utils.norm_dist(softmax(np.array([0, 0.25, 0.75])))
# For action modality, have agent prefer non-NULL actions equally
C[num_modalities-1] = utils.norm_dist(np.array([0.2,0.2,0.2,0.2,0.2,0.0]))
print(f"utils.is_normalized(C) = {utils.is_normalized(C)}")

# PRIOR BELIEFS ABOUT STATES P(s) -- D: uniform prior belief about quality
D = utils.obj_array(num_factors)
D[0] = utils.norm_dist(np.array([1.0, 0.0]))
#D[0] = utils.norm_dist(np.ones(shape=num_states))
print(f"utils.is_normalized(D) = {utils.is_normalized(D)}")

# HABITS P(pi) -- E: uniform (these do not change, thus pertually uniform habits will not contribute to policy inference)
E = utils.norm_dist(np.ones(shape=num_controls))
print(f"utils.is_normalized(E) = {utils.is_normalized(E)}")


pA = utils.dirichlet_like(A, scale = 1.0)
pB = utils.dirichlet_like(B, scale = 1.0)
pD = utils.dirichlet_like(D, scale = 1.0)

agent = Agent(A = A, B = B, C = C, D = D, E = E, pA = pA, pB = pB, pD = pD,
              inference_algo = 'MMP', policy_len=1,
              inference_horizon=2, sampling_mode = 'full',
              action_selection = 'stochastic')


#obs = [literature_obs_names, novelty_obs_names, safety_obs_names, methods_obs_names, synthesizer_obs_names, last_action_taken_obs_names]  # track last action for B to update

# store original qs
qs_prev = agent.D
# Hard-code an observation (0=poor quality for each category; 5=NULL action taken last step)
obs = [0,0,0,0,0,5]
# Infer states, given the observation
qs = agent.infer_states(obs)
print(f"agent.F = {agent.F}") # see the VFE per policy

action_sampled_id = 5   # NULL action

####qs_prev = copy.deepcopy(qs[action_sampled_id][0])  # store 1D qs
print(f"qs_prev = {qs_prev}")

# Learning A
# # MMP produces more complex qs structure (per action and t in horizon) than the 'VANILLA' algorithm, therefore...
agent.qs = copy.deepcopy(qs[action_sampled_id][0])   #...we extract only the 1D vector of qs given the agent's action taken + current timestep and set it temporarily as the agent's internal qs...
agent.update_A(obs)                                  #...which is used, with the received observations, to update agent's likelihood model internally using update_A()
agent.qs = copy.deepcopy(qs)                         #...and then we can set the agent's internal qs back to the full MMP output
#print(f"updated A = {agent.A}")

# Infer policies and choose an action
q_pi, neg_efe = agent.infer_policies()
action_sampled_id = int(agent.sample_action()[0])
print(f"agent.sample_action() = {action_sampled_id}")

# Learning B
# - similarly to A, update_B() requires 1D qs--here at previous timestep, i.e. the 's_t' component of `B = P(s_t+1 | s_t, pi)`
# - requires that agent.action, populated by via sample_action(), is not None thus why we run this AFTER sampling actions
agent.qs = copy.deepcopy(qs[action_sampled_id][0])
print(f"agent.action = {agent.action}")
agent.update_B(qs_prev)
agent.qs = copy.deepcopy(qs)
#print(f"updated B = {agent.B}")
print(f"P(s_t+1=high|s_t=low,action=NULL) = {agent.B[0][1,0,5]}")
#### On learning B formula: `B = softmax(precision_B * pB)

# Learning D
# - unlike update_A() and update_B(), update_D() has an internal protocol for dealing with MMP-based agents
#   by using the model average of the MMP-based qs...
print(qs[action_sampled_id][0])
agent.update_D()  # ...thus we simply call update_D()
print(f"updated D = {agent.D}")   # !!!!! NOTE: Needs further testing / iterations to see behavior of updates in relation to method logic

#print(literature_obs_names, novelty_obs_names, safety_obs_names, methods_obs_names, synthesizer_obs_names, last_action_taken_obs_names)

# Experiment 2: Initial phase -- Each role constructs an initial section of the paper and self-grades
from openai import OpenAI

# Initialize the OpenAI client
client = OpenAI(api_key=YOUR_API_KEY)

# Set seed for reproducibility
seed = 1

# Framework/field from which agents are working
field = "Active Inference"
# Topic/phenomena of interest
phenomena = "the neurobiological and subjective phenomenological aspects of PTSD and therapeutic treatment interventions"

# Agent types: one for each agent type
agent_types = ["literature",
               "novelty",
               "safety",
               "methods",
               "synthesizer"]
# Agent role substring: one for each agent type
roles = ["literature on",
         "novel uses of",
         "ethics, safety concerns, and risk mitigation of projects in",
         "methodology and and key concepts of",
         "synthesis of information into research papers and then reviewing their coherency in"]
role_prompts = []
for i in range(len(roles)):
  role_prompts.append(f"""You are an expert in the {roles[i]} {field}. You are working on a project which applies the methods of {field} to the study of {phenomena} in a novel way.
  You have are responsible for the section of the paper with the header [[{agent_types[i]}]], where double brackets denote headers.""")

def generate_initial_prompt(i, roles, field, phenomena, full_text):
  if i == 0:   # Literature
    initial_prompt = f"""Write a brief overview of the {roles[i]} {field} and its potential uses for the study of {phenomena} using only real sources. Your sources absolutely must be real otherwise the paper will be terrible and dishonest, ruining the entire project.
    Include citations to the references you use. You are only working on the
          literature section so do not include any additional comments beyond the literature review.
          Preface your literature review with the following string inside the tickets: `[[literature]]`.
          At the very end of your response, grade how well your literature review could contribute to novel applications of {field} to {phenomena} on a scale of 1 to 3 enclosed in double brackets,
          for example [[1]] to mean 'poorly'
          """
  elif i == 1:  # Novelty
    initial_prompt = f"""Your team's current working draft contains the following sections, where each section begins with its name enclosed in double brackets: {full_text}.
          Propose an idea for an experiment from which we could derive insights to add to the existing literature on {phenomena} from a {field} perspective.
          You are only working on the methods section so do not include any additional commentary beyond the methods section.
          Preface your novel idea proposal with the following string, including the double brackets, inside the ticks: `[[{agent_types[i]}]]`
          At the very end of your response, grade how well your experiment idea contributes to novel applications of {field} to {phenomena} on a scale of 1 to 3 enclosed in double brackets,
          for example [[1]] to mean 'poorly'."""
  elif i == 2:  # Safety
    initial_prompt = f"""Your team's current working draft contains the following sections, where each section begins with its name enclosed in double brackets: {full_text}.
          Write a review of safety and ethical concerns as relates to the study of {phenomena} as well as research in the field of {field}.
          Then write a three paragraph statement on these concerns for researchers who intend to develop research and test experiments in the study of {phenomena} in the {field} field.
          Preface your review with the following string, including the double brackets, inside the ticks: `[[{agent_types[i]}]]`
          At the very end of your response, grade how well your safety statement contributes to maintaining ethical standards in novel applications of {field} to {phenomena} on a scale of 1 to 3 enclosed in double brackets,
          for example [[1]] to mean 'poorly'"""
  elif i == 3:  # Methods
    initial_prompt = f"""Write a three paragraph Methods statement as a starting point for your paper on the study of {phenomena} in {field}. The first paragraph should list typical ways that the methodology and key concepts of {field} are used in research.
          The second paragraph should describe existing typical methods of studying the phenomena of {phenomena}.
          The third paragraph should synthesize the information in the previous two paragraphs to determine different ways the methodology of {field} can be applied to the study of {phenomena}.
          Preface your methods statement with the following string, including the double brackets, inside the ticks: `[[{agent_types[i]}]]`
          At the very end of your response, grade how well your methods section contributes to a starting point for guiding novel applications of {field} to {phenomena} on a scale of 1 to 3 enclosed in double brackets,
          for example [[1]] to mean 'poorly'"""
  elif i == 4: # Synthesizer
    initial_prompt = f"""Your team's current working draft contains the following sections, where each section begins with its name enclosed in double brackets: {full_text}.
          Your research team has developed a compilation of sections for developing the research paper on applying {field} to research on {phenomena}.
          Each section begins with a phrase in double brackets.
          The compilation of sections is as follows: `{full_text}`. Write an abstract for the research paper which synthesizes this information.
          Preface your abstract with the following string, including the double brackets, inside the ticks: `[[{agent_types[i]}]]`
          At the very end of your response, grade the consistency and coherency of the full compilation of sections, with your abstract included, as they apply to developing a novel application of {field} to {phenomena} on a scale of 1 to 3 enclosed in double brackets,
          for example [[1]] to mean 'poorly'"""

  #initial_prompt = initial_prompt + "\n At the end, grade your own work on a scale of 1 to 3 enclosed in double brackets, for example [[1]] to mean 'poorly'."
  return initial_prompt

def generate_check_prompt(i, roles, field, phenomena, full_text, agent_types):
  check_prompt = f"""Your research team's working draft is now as follows, where strings in double brackets denote the start of a section:
                      `{full_text}`. You are responsible for the `[[{agent_types[i]}]]` section.
                      First, determine how consistent the working draft is with {roles[i]} {field}, on {phenomena}, and applying {field} to the study of {phenomena},
                      and then grade the draft from standpoint on a scale of 1 to 3 enclosed in double brackets, for example [[1]] to mean 'poor', [[2]] to mean 'medium', [[3]] to mean 'very good'.
                      Your response should only be your chosen grade enclosed in double brackets. Do not include any commentary or words.
                      """
                      #and write a paragraph statement which critiques the draft from the standpoint of your determination so that your research team can take the critique into account for making further progress.
                      #At the end of your paragraph, grade the draft from this standpoint on a scale of 1 to 3 enclosed in double brackets, for example [[1]] to mean 'poorly'."""
  return check_prompt

def generate_action_prompt(action_sampled_id, roles, field, phenomena, full_text, agent_types, obs):
  grades_description = ""
  # Concatenate, for each LLM role, their section name, role, and previous grade.
  for role in range(len(obs)-1):   # -1: do not include action observation, i.e. the final observation in the list
    grades_description = grades_description + f"The [[{agent_types[role]}]] section, pertaining to the {roles[role]} applying {field} to {phenomena}, received a grade of {obs[role]}.  "
  i = action_sampled_id
  action_prompt = f"""The current working draft of your research team's paper is: `{full_text}`. You are still working on the [[{agent_types[i]}]] section which begins with the string [[{agent_types[i]}]] and ends when the next double bracket enclosed string begins.
                      Your research team has collectively evaluated the draft, where each section received a graded on a scale of 1 to 3, with `1` denoting 'poor' and heavy revision needed,
                      `2` denoting average quality and slight revisions needed, and `3` denoting high quality and minor revisions needed to make the literature review fit the updated direction of the paper. The grades are as follows: {grades_description}.
                      Make adjustments to your contributed section, the [[{agent_types[i]}]] section, based on the grade you received. Respond only with your adjusted section as it should appear in the final paper.
                      Do not include any further commentary or explanation because your section will go into the final paper and be published and therefore should not contain any superfluous statements.
                      Preface your response with the string in ticks: `[[{agent_types[i]}]]`. Also, if you are working on the `[[literature]]` section, be sure to include a list of all references you have used, properly cited in APA format, and make sure the sources are real sources and not fictional sources.
                      If you are not working on the [[literature]] section then ignore the last sentence.
                    """
  return action_prompt

project = {}       # initialize 'project' for storing information
project[0] = {}    # initialize first time step
full_text = ""     # initialize full_text (string containing the paper)
obs = []
obs_list = []
action_id_list = []

temperature = 0.9    # temperature parameter for LLM     #!!!! could modulate based on agent action choices

# Initial phase ----------------------------------------------------------------------------------------------------------------------------
for i in range(len(agent_types)):   # for each agent...
  print(f"i={i} agent_type {agent_types[i]}")
  # Send the request to the GPT-4o mini model based on which agent
  response = client.chat.completions.create(
      model="gpt-4o-mini",
      messages=[
          {"role": "system", "content": role_prompts[i]},
          {"role": "user", "content": generate_initial_prompt(i=i, roles=roles, field=field, phenomena=phenomena, full_text=full_text)}
      ],
      temperature=temperature,  # controls randomness vs. coherence
      seed = seed
  )
  # Extract score
  import re
  match = re.search(r'\[\[(\d+)\]\]', str(response.choices[0].message.content))
  extracted_score = match.group(1) if match else None
  extracted_score = int(extracted_score) - 1     # convert and align score value/type ('1','2','3') to observation encoding 0,1,2
  # Extract response without grade
  extracted_response = re.sub(r'\[\[\d+\]\]', '', str(response.choices[0].message.content)).strip()
  full_text = full_text + "\n" + extracted_response  # Append extracted response to full_text
  # Log score, response, and current version of full_text
  project[0][agent_types[i]] = {'score' : extracted_score, 'response' : extracted_response}
  obs.append(project[0][agent_types[i]]['score'])
  print(f"i={i} agent_type {agent_types[i]} self-graded extracted_score {extracted_score}")

obs.append(len(agent_types))  # append the 'NULL' action observation for the first time step
action_id_list.append(len(agent_types))  # append the NULL action for tracking purposes
obs_list.append(obs)                         # append the full observation list for tracking purposes


## <Start revision block> Initialize as everything being of 'poor' quality #######################
## REMOVE/COMMENT-OUT this block to use self-grading results as initial observation
obs = []
obs_list = []
for i in range(len(agent_types)):
  project[0][agent_types[i]]['score'] = 0
  obs.append(0)
obs.append(len(agent_types))
obs_list.append(obs)
## <End revision block> ##########################################################################

print(f"obs for start of action-perception loop = {obs}")
# Copy all results to be used for action-perception loop
initial_full_text = full_text
initial_project = project
initial_obs = copy.deepcopy(obs)
initial_obs_list = copy.deepcopy(obs_list)
initial_action_id_list = copy.deepcopy(action_id_list)

# Experiment 2: Action-Perception Loop

# Retrieve all products and logging lists from initial phase
full_text = initial_full_text
project = project = initial_project
obs = copy.deepcopy(initial_obs)
obs_list = copy.deepcopy(initial_obs_list)
action_id_list = copy.deepcopy(initial_action_id_list)

# Initialize agent-specific tracking lists
efe_list = [[np.nan] * len(action_names)]    # EFE has not yet been calculated thus null
vfe_list = [[np.nan] * len(action_names)]    # VFE has not yet been calculated thus null
q_pi_list = [list(agent.E)]                        # q_pi hasn't been calculated yet thus = habits (E vector)
qs_list = [list(agent.D)]                          # qs hasn't been calculated yet thus = priors (D vector)
d_list = [list(agent.D)]
B_list = [agent.B]


# Initializing agent and particular priors before inference
agent = Agent(A = A, B = B, C = C, D = D, E = E, pA = pA, pB = pB, pD = pD,
              inference_algo = 'MMP', policy_len=1,
              inference_horizon=2, sampling_mode = 'full',
              action_selection = 'stochastic')
# store original qs
qs_prev = agent.D
# begin with 'NULL' action
action_sampled_id = 5   # NULL action

temperature = 0.3

# Action-perception loop
T = 10
for t in range(1,T+1): # For second time step (indexed at 1) onwards (range excludes the final integer reached, thus the +1)
  print(f"t={t}-------------------")
  print(f"obs = {obs}")
  project[t] = project[t-1]    # initialize new project for current timestep as a copy of last timestep's project
  #print(f"Starting inference phase")
  qs = agent.infer_states(obs)

  # Learning A
  # # MMP produces more complex qs structure (per action and t in horizon) than the 'VANILLA' MP algorithm, therefore...
  agent.qs = copy.deepcopy(qs[action_sampled_id][0])   #...we extract only the 1D vector of qs given the agent's action taken + current timestep and set it temporarily as the agent's internal qs...
  agent.update_A(obs)                                  #...which is used, with the received observations, to update agent's likelihood model internally using update_A()
  agent.qs = copy.deepcopy(qs)                         #...and then we can set the agent's internal qs back to the full MMP output
  #print(f"updated A = {agent.A}")
  vfe_list.append(list(agent.F))
  qs_list.append(list(qs[action_sampled_id][0]))

  # Infer policies and choose an action
  q_pi, neg_efe = agent.infer_policies()                 # infer policies
  action_sampled_id = int(agent.sample_action()[0])      # choose policy (action) to take next
  print(f"agent.sample_action() = {action_sampled_id} ({action_names[action_sampled_id]})")
  efe_list.append(list(neg_efe*-1))
  q_pi_list.append(list(q_pi))

  # Learning B
  # - similarly to A, update_B() requires 1D qs--here at previous timestep, i.e. the 's_t' component of `B = P(s_t+1 | s_t, pi)`
  # - requires that agent.action, populated by via sample_action(), is not None thus why we run this AFTER sampling actions
  agent.qs = copy.deepcopy(qs[action_sampled_id][0])
  #print(f"agent.action = {agent.action}")
  agent.update_B(qs_prev)
  agent.qs = copy.deepcopy(qs)
  B_list.append(agent.B)
  print(f"updated agent.B[0][1,0,:] = P(s_t+1=high|s_t=low,pi) = {agent.B[0][1,0,:]}")
  #### On learning B formula: `B = softmax(precision_B * pB)

  # Learning D
  # - unlike update_A() and update_B(), update_D() has an internal protocol for dealing with MMP-based agents
  #   by using the model average of the MMP-based qs...
  #print(qs[action_sampled_id][0])
  agent.update_D()  # ...thus we simply call update_D()
  d_list.append(list(agent.D))
  print(f"updated D = {agent.D}")   # !!!!! NOTE: Needs further testing / iterations to see behavior of updates in relation to method logic

  # Action phase-----------------------------------------------------------------------------------
  if action_sampled_id != len(action_names)-1:   # If agent chose to modify a section (i.e. did not choose the final action 'NULL'; -1 because of 0-indexing)
    #project[t] = project[t-1]    # copy previous project to new timestep project log
    i = action_sampled_id
    # Generate action prompt for the chosen section to modify
    response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": role_prompts[i]},
        {"role": "user", "content": generate_action_prompt(action_sampled_id=action_sampled_id, roles=roles, field=field, phenomena=phenomena, full_text=full_text, agent_types=agent_types, obs=obs)}
    ],
    temperature=temperature,  # controls randomness vs. coherence
    seed = seed
    )
    action_response = str(response.choices[0].message.content)

    # Replace section in the project accordingly
    project[t][agent_types[action_sampled_id]]['response'] = action_response    # replace response for section chosen
    # Redefine full_text using all current sections
    full_text = ""
    for agent_type in agent_types:
      full_text = full_text + "\n" + project[t][agent_type]['response']

  # Check phase-------------------------------------------------     !!run thru the relevant parts of the Check phase code below, generate obs, append action_id at end
  # initialize new observation list
  obs = []
  for i in range(len(agent_types)):   # for each agent...
    #print(f"i={i} agent_type {agent_types[i]}")
    # Send the request to the GPT-4o mini model based on which agent
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": role_prompts[i]},
            {"role": "user", "content": generate_check_prompt(i, roles=roles, field=field, phenomena=phenomena, full_text=full_text, agent_types=agent_types)}
        ],
        temperature=temperature,  # controls randomness vs. coherence
        seed = seed
    )
    # Extract score
    import re
    match = re.search(r'\[\[(\d+)\]\]', str(response.choices[0].message.content))
    extracted_score = match.group(1) if match else None
    extracted_score = int(extracted_score) - 1
    ## Extract response without grade
    #extracted_response = re.sub(r'\[\[\d+\]\]', '', str(response.choices[0].message.content)).strip()
    #full_text = full_text + "\n" + extracted_response  # Append extracted response to full_text
    # Log score, response, and current version of full_text
    project[0][agent_types[i]]['score'] =extracted_score
    obs.append(project[0][agent_types[i]]['score'])    # record observation per agent type
    print(f"i={i} agent_type {agent_types[i]} extracted_score {extracted_score}")  # current obs {obs}")

  obs.append(action_sampled_id)  # append action chosen at the end of the observation list
  obs_list.append(obs)           # record full obs list
  action_id_list.append(action_sampled_id)
  print(f"obs for t+1 = {obs}")

print(f"EFE over time:")
display(pd.DataFrame(efe_list, columns=action_names))

print(f"VFE over time:")
display(pd.DataFrame(vfe_list, columns=action_names))


print(f"Observations over time:")
display(pd.DataFrame(obs_list, columns=action_names))

import copy
B_list_bkp = copy.deepcopy(B_list)

B_list = copy.deepcopy(B_list_bkp)
print(B_list_bkp[0][0])

import matplotlib.pyplot as plt
import numpy as np

actions = ["literature", "novelty", "safety", "methods", "synthesizer", "NULL"]
transitions = ["low to low", "low to high", "high to low", "high to high"]
colors = ['#8B4513', '#87CEFA', '#FFA500', '#90EE90']  # Brown, Light Blue, Orange, Light Green

# Determine the number of timesteps
num_timesteps = len(B_list)

# Create a figure with subplots for each action
fig, axes = plt.subplots(len(actions), 1, figsize=(12, 2*len(actions)), sharex=True)
fig.suptitle('State Transition Beliefs Over Time', fontsize=16)

for a, action in enumerate(actions):
    ax = axes[a]

    # Initialize arrays to store probabilities over time
    probs_over_time = np.zeros((4, num_timesteps))

    for t in range(num_timesteps):
        probs_over_time[0, t] = B_list[t][0][0, 0, a]  # low to low
        probs_over_time[1, t] = B_list[t][0][1, 0, a]  # low to high
        probs_over_time[2, t] = B_list[t][0][0, 1, a]  # high to low
        probs_over_time[3, t] = B_list[t][0][1, 1, a]  # high to high

    # Plot lines for each transition type with opacity set to 0.5
    for i, transition in enumerate(transitions):
        ax.plot(range(num_timesteps), probs_over_time[i, :], label=transition, color=colors[i], marker='o', alpha=0.6)

    ax.set_ylabel(f'{action}\nProb', fontsize=8)
    ax.set_ylim(0, 1)
    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=8, title='P(s_t+1|s_t,pi)')
    ax.grid(True, linestyle='--', alpha=0.7)
    #ax.set_title(action, fontsize=10)
    ax.tick_params(axis='both', which='major', labelsize=8)

plt.xlabel('Timestep', fontsize=10)
plt.tight_layout()
plt.show()

print(B_list[0])
print(B_list[len(B_list)-1])

import numpy as np
import matplotlib.pyplot as plt

# Convert lists to numpy arrays for easier handling
efe_data = np.array(efe_list)
vfe_data = np.array(vfe_list)
q_pi_data = np.array(q_pi_list)
qs_data = np.array([q[0][1] for q in qs_list])  # Extract 'good' quality metric
d_data = np.array([d[0][1] for d in d_list])

# Create subplots horizontally
fig, axs = plt.subplots(1, 5, figsize=(25, 6))
fig.suptitle('', fontsize=16)

# X-axis values (timesteps)
timesteps = range(len(qs_list))

# Define a list of distinct colors and line styles
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#7f7f7f']
line_styles = ['-', '--', '-.', ':', '-', '--']

# Function to plot data and action dots
def plot_data_and_actions(ax, data, title, ylabel, is_single_line=False):
    if is_single_line:
        ax.plot(timesteps, data, label='Quality', color='blue', linewidth=2)
    else:
        for i, action in enumerate(action_names):
            ax.plot(timesteps, data[:, i], label=action, color=colors[i], linestyle=line_styles[i], linewidth=2)

    y_min, y_max = ax.get_ylim()
    dot_y = y_min - 0.0005 * (y_max - y_min)  # Place dots slightly above the y-axis minimum

    for t, action_id in enumerate(action_id_list):
        ax.scatter(t, dot_y, color=colors[action_id], s=200, zorder=5)

    ax.set_ylim(bottom=y_min - 0.001 * (y_max - y_min))  # Adjust y-axis to show dots clearly
    ax.set_ylabel(ylabel, fontsize=12)
    ax.set_xlabel('Timestep', fontsize=12)
    ax.legend(fontsize=10)
    ax.set_title(title, fontsize=14)
    ax.grid(True, alpha=0.3)

# Plot EFE data
plot_data_and_actions(axs[0], efe_data, 'Expected Free Energy (EFE)', 'EFE Value')

# Plot VFE data
plot_data_and_actions(axs[1], vfe_data, 'Variational Free Energy (VFE)', 'VFE Value')

# Plot Q(pi) data
plot_data_and_actions(axs[2], q_pi_data, 'Q(pi)', 'Q(pi) Value')

# Plot 'Good' Quality metric
plot_data_and_actions(axs[3], qs_data, 'Quality Q(s=high)', 'Value', is_single_line=True)

#
plot_data_and_actions(axs[4], d_data, 'D P(s0)', 'P(s0)', is_single_line=True)

# Adjust layout and display the plot
plt.tight_layout()
plt.show()

# Prepare final text for export (give it a title, restructure order, export to pdf)
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": f"You are an academic who is familiar with the terminology and concepts used in {field} and as well as in the study of {phenomena}"},
        {"role": "user", "content": f"""Read the following scientific paper enclosed in ticks: `{full_text}`. What is a professional yet clever title for this paper? Only include the title in your response. Do not add any additional commentary."""}
    ],
    temperature=temperature,  # controls randomness vs. coherence
    seed = seed
)
full_text_final = response.choices[0].message.content + '\n \n \n'
for role in ['synthesizer','literature','novelty','methods','safety']:
  full_text_final = '\n' + full_text_final + project[T][role]['response'] + '\n \n'
print(full_text_final)

!pip install fpdf

# Export full_text to pdf
from fpdf import FPDF
import textwrap

class PDF(FPDF):
    def __init__(self):
        super().__init__()
        self.set_auto_page_break(auto=True, margin=15)

    def chapter_body(self, body):
        self.set_font("Times", size=12) #"B", size=12)
        self.set_left_margin(10)
        self.set_right_margin(10)

        paragraphs = body.split('\n')
        for paragraph in paragraphs:
            lines = textwrap.wrap(paragraph, width=80)  # Adjust width as needed
            if lines:
                for line in lines:
                    self.cell(0, 5, txt=line.encode('latin-1', 'replace').decode('latin-1'), ln=1)
            else:
                self.ln()  # Empty line

def create_pdf_from_string(text, output_filename):
    pdf = PDF()
    pdf.add_page()
    pdf.chapter_body(text)
    pdf.output(output_filename, 'F')

# Create the PDF
create_pdf_from_string(full_text_final, "research_paper.pdf")